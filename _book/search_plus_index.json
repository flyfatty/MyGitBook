{"./":{"url":"./","title":"Introducation","keywords":"","body":""},"book/tutorial/markdown.html":{"url":"book/tutorial/markdown.html","title":"MarkDown","keywords":"","body":"主要特性 支持“标准”Markdown / CommonMark和Github风格的语法，也可变身为代码编辑器； 支持实时预览、图片（跨域）上传、预格式文本/代码/表格插入、代码折叠、搜索替换、只读模式、自定义样式主题和多语言语法高亮等功能； 支持ToC（Table of Contents）、Emoji表情、Task lists、@链接等Markdown扩展语法； 支持TeX科学公式（基于KaTeX）、流程图 Flowchart 和 时序图 Sequence Diagram; 支持识别和解析HTML标签，并且支持自定义过滤标签解析，具有可靠的安全性和几乎无限的扩展性； 支持 AMD / CMD 模块化加载（支持 Require.js & Sea.js），并且支持自定义扩展插件； 兼容主流的浏览器（IE8+）和Zepto.js，且支持iPad等平板设备； 支持自定义主题样式； Editor.md 目录 (Table of Contents) [TOCM] [TOC] 科学公式 TeX笔记 TeX编辑器 E=mc^2 行内的公式E=mc^2行内的公式 (\\sqrt{3x-1}+(1+x)^2) \\sin(\\alpha)^{\\theta}=\\sum_{i=0}^{n}(x^i + \\cos(f)) Heading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Heading 1 link Heading link Heading 2 link Heading link Heading 3 link Heading link Heading 4 link Heading link Heading link Heading link Heading 5 link Heading link Heading 6 link Heading link 标题（用底线的形式）Heading (underline) This is an H1 This is an H2 字符效果和横线等 删除线 删除线（开启识别HTML标签时） 斜体字 斜体字 粗体 粗体 粗斜体 粗斜体 上标：X2，下标：O2 缩写(同HTML的abbr标签) 即更长的单词或短语的缩写形式，前提是开启识别HTML标签时，已默认开启 The HTML specification is maintained by the W3C. 引用 Blockquotes 引用文本 Blockquotes 引用的行内混合 Blockquotes 引用：如果想要插入空白换行即标签，在插入处先键入两个以上的空格然后回车即可，普通链接。 锚点与链接 Links 普通链接 普通链接带标题 直接链接：https://github.com 锚点链接 GFM a-tail link @pandao @pandao 多语言代码高亮 Codes 行内代码 Inline code 执行命令：npm install marked 缩进风格 即缩进四个空格，也做为实现类似预格式化文本(Preformatted Text)的功能。 预格式化文本： | First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell | JS代码　 function test(){ console.log(\"Hello world!\"); } (function(){ var box = function(){ return box.fn.init(); }; box.prototype = box.fn = { init : function(){ console.log('box.init()'); return this; }, add : function(str){ alert(\"add\", str); return this; }, remove : function(str){ alert(\"remove\", str); return this; } }; box.fn.init.prototype = box.fn; window.box =box; })(); var testBox = box(); testBox.add(\"jQuery\").remove(\"jQuery\"); HTML代码 HTML codes Hello world! Hello world! 图片 Images Image: Follow your heart. 图为：厦门白城沙滩 图片加链接 (Image + Link)： 图为：李健首张专辑《似水流年》封面 列表 Lists 无序列表（减号）Unordered Lists (-) 列表一 列表二 列表三 无序列表（星号）Unordered Lists (*) 列表一 列表二 列表三 无序列表（加号和嵌套）Unordered Lists (+) 列表一 列表二 列表二-1 列表二-2 列表二-3 列表三 列表一 列表二 列表三 有序列表 Ordered Lists (-) 第一行 第二行 第三行 GFM task list [x] GFM task list 1 [x] GFM task list 2 [ ] GFM task list 3 [ ] GFM task list 3-1 [ ] GFM task list 3-2 [ ] GFM task list 3-3 [ ] GFM task list 4 [ ] GFM task list 4-1 [ ] GFM task list 4-2 绘制表格 Tables 项目 价格 数量 计算机 $1600 5 手机 $12 12 管线 $1 234 First Header Second Header Content Cell Content Cell Content Cell Content Cell First Header Second Header Content Cell Content Cell Content Cell Content Cell Function name Description help() Display the help window. destroy() Destroy your computer! Left-Aligned Center Aligned Right Aligned col 3 is some wordy text $1600 col 2 is centered $12 zebra stripes are neat $1 Item Value Computer $1600 Phone $12 Pipe $1 特殊符号 HTML Entities Codes © & ¨ ™ ¡ £ & ¥ € ® ± ¶ § ¦ ¯ « · X² Y³ ¾ ¼ × ÷ » 18ºC \" ' Emoji表情 Blockquotes 参考链接 反斜杠 Escape *literal asterisks* 绘制流程图 Flowchart graph TD; A-->B; A-->C; B-->D; C-->D; 绘制序列图 Sequence Diagram sequenceDiagram participant Alice participant Bob Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts prevail! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! 绘制甘特图 Gantt gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d "},"book/tutorial/ml/basic.html":{"url":"book/tutorial/ml/basic.html","title":"什么是机器学习","keywords":"","body":"什么是机器学习 机器学习的定义 computer program is said to learn from experience E with respect to some task T and some performance measure P，if its performance on T，as measured by P，improves with experience E. [Tom Mitchell,1998] E(Experience) 经验 T(Task) 任务 P(Performance) 性能 假设P来评估计算机程序在某任务类T上的性能，若一个程序通过利用经验E在T中任务上获得了性能改善，则关于T和P，该程序对E进行了学习。 机器学习的主要任务 给定在现实中获取的一系列数据来构建一个模型,用于描述数据的具体分布（概率意义上的分布） 数据 graph LR; A(数据)--有标签-->B[监督学习]; A--无标签-->C[无监督学习]; A--环境-->G[强化学习]; B-->D[回归]; B-->E[分类]; C-->F[聚类] 模型 我们可以认为机器学习模型是一个概率分布P_\\theta(X)，其中X表示训练数据,机器学习的任务就是求解最优参数\\theta_{t}的过程 极大似然估计（MLE） \\theta_{t}=\\underset{\\theta}{\\mathrm{argmax}}P_{\\theta}(X) 假设训练数据的单条数据之间相互独立 \\theta_{t}=\\underset{\\theta}{\\mathrm{argmax}}\\prod_{i}P_{\\theta}(X_{i})=\\underset{\\theta}{\\mathrm{argmax}}\\sum_{i}\\log P_{\\theta}(X_{i}) 机器学习的方法论 三要素 graph TD M(模型)-->P(策略); P-->S(算法); S-->M; 一般步骤 得到一个有限的训练数据集合 确定包含所有可能的模型的假设空间，即学习模型的集合 确定模型选择的准则，即学习的策略（比如损失函数） 实现求解最优模型的算法，即学习的算法（比如梯度下降） 通过学习算法选择最优模型 利用学习的最优模型对新数据进行预测和分析 生成模型和判别模型 对于监督学习，训练集包含输入的数据X和对应的标签Y。所以我们应该求的概率分布有两种，即联合概率分布P_{\\theta}(X,Y)和条件概率分布P_{\\theta}(Y|X),前者表示数据和标签共现概率，后者表示在给定数据的条件下对应的标签的概率。根据描述的概率分布，我们也可以把监督学习的机器学习模型分为生成式模型和判别式模型。 graph LR; A(监督学习)--联合概率-->B[生成式模型]; A--条件概率-->C[判别式模型]; B-->D[朴素贝叶斯]; B-->E[隐马尔可夫模型]; C-->F[逻辑回归]; C-->G[条件随机场] 生成模型不仅可以根据输入X预测标签Y，还能根据训练得到的模型产生服从训练数据集分布的数据(X,Y)，相当于生成一组新的数据。 P(Y|X)=\\frac{P(X,Y)}{P(X)} 判别模型仅能根据输入X预测标签Y。牺牲了生成数据的能力，获取的是比生成模型高的预测准确率。 定性解释 根据全概率公式，相对条件概率，在计算联合概率的时候引入了输入数据的概率分布P(X)，而这不是我们关心的，于是削弱了模型的预测能力。 P(X,Y)=\\int P(Y|X)P(X)\\text{d}X 定量解释 根据信息熵公式，如果概率密度相对集中，则包含信息越少，信息熵就越小。 H(X)=-\\int P(X)\\log P(X)\\text{d}X import seaborn as sns import numpy as np sns.set_style(\"darkgrid\") p = np.linspace(0,1,100) h = -p*np.log2(p)-(1-p)*np.log2(1-p) sns.relplot(x=p,y=h,kind='line',color=\"r\") 定义联合分布的信息熵和条件分布的信息熵，由于H(X)\\geq 0，可以得到条件分布的信息熵小于等于联合分布。对应到模型上可以认为判别模型比生成模型包含更多的信息。 H(X,Y)=-\\int P(X,Y)\\log P(X,Y)\\text{d}X\\text{d}Y H(Y|X)=-\\int P(X,Y)\\log P(Y|X)\\text{d}X H(Y|X)=H(X,Y)-H(X) 如何优化机器学习模型 回归模型 假设噪声f_{\\theta}(X_i)-Y_i服从高斯分布\\mathbb{N}(0,\\sigma^{2})，这样就可以通过极大似然估计的方法计算最优参数\\theta_t P(X) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{({X-\\mu})^2}{2\\sigma^2}} 根据高斯公式，最大化条件概率分布P(Y|X)等价于最小化噪音的概率分布。 \\begin{align} \\theta_{t}&=\\underset{\\theta}{\\mathrm{argmax}}\\sum_i\\log P_{\\theta}(X_i) \\\\ &=\\underset{\\theta}{\\mathrm{argmax}}\\sum_i\\log\\frac{1}{\\sqrt{2\\pi}\\sigma }e^{-\\frac{(f_\\theta(X_i)-Y_i)^2}{2\\sigma^2}} \\\\ &=\\underset{\\theta}{\\mathrm{argmin}}\\sum_i{(f_\\theta(X_i)-Y_i)^2} \\end{align} 我们把argmin函数中需要优化的函数成为损失函数，该函数被称为L2损失函数或MSE均方误差损失。 假设误差服从拉普拉斯分布 f(\\varepsilon )=\\frac{1}{2\\lambda}e^{-\\frac{\\left |\\varepsilon -\\mu\\right |}{\\lambda}} \\theta_{t}=\\underset{\\theta}{\\mathrm{argmin}}\\sum_i\\left | f_\\theta(X_i)-Y_i\\right | 为了消除过拟合的状态，需要引入参数的先验分布，比如高斯分布，这样就能减小参数的选择范围。 \\begin{align} \\theta_{t}&=\\underset{\\theta}{\\mathrm{argmax}}\\sum_i\\log P_\\theta(X_i|\\theta)P(\\theta) \\\\ &=\\underset{\\theta}{\\mathrm{argmin}}\\sum_i(f_\\theta(X_i)-Y_i)^2+\\frac{\\alpha}{2}{\\parallel \\theta\\parallel}^2 \\end{align} 分类模型 "},"book/tutorial/ml/rnn.html":{"url":"book/tutorial/ml/rnn.html","title":"循环神经网络","keywords":"","body":"RNN 循环神经网络 为了处理时序数据并利用其历史信息，我们需要让网络具有短期记忆能力，而前馈网络是一种静态网络，不具备这种记忆能力。 经典模型 Simple RNN h_t=f(Uh_{t-1}+Wx_t+b) LSTM 学习的参数可以看作长时记忆，h_t看作短时记忆（外部记忆状态），c_t看作比较长的短时记忆（内部记忆状态），即Long Short-Term Memory。 LSTM执行的步骤如下： 利用上一时刻的外部状态h_{t-1}和当前时刻的输入x_t计算出三个门（输入门、输出门、遗忘门）以及候选状态\\widetilde{c}_t。 结合遗忘门f_t和输入门i_t更新记忆单元c_t。 结合输出门o_t，将内部状态的信息传递给外部状态h_t。 \\begin{align} \\begin{bmatrix} \\widetilde{c}_t\\\\ o_t\\\\ i_t\\\\ f_t \\end{bmatrix}&= \\begin{bmatrix} \\tanh\\\\ \\sigma\\\\ \\sigma\\\\ \\sigma \\end{bmatrix}\\left ( \\mathbf{W}\\begin{bmatrix} x_t\\\\ h_{t-1} \\end{bmatrix} + b \\right ) \\\\ c_t&=f_t\\odot c_{t-1}+i_t\\odot\\widetilde{c}_t\\\\ h_t&=o_t\\odot\\tanh\\left ( c_t \\right ) \\end{align} 训练经验 : 遗忘门的参数初始值一般都设置得比较大，其偏置向量b_f设为1或2。过小的值意味着刚训练时遗忘门的概率值过小，大部分信息都会丢失，而且梯度也会非常小，从而导致梯度消失问题。 LSTM变体 无遗忘门的LSTM_{1997} c_t=c_{t-1}+i_t\\odot\\widetilde{c}_t peephole 输入不止依赖上一时刻的隐状态h_{t-1}，也依赖上一时刻的记忆单元c_{t-1} \\begin{align} i_t&=\\sigma\\left ( W_ix_t+U_ih_{t-1}+V_ic_{t-1}+b_i\\right ) \\\\ f_t&=\\sigma\\left ( W_fx_t+U_fh_{t-1}+V_fc_{t-1}+b_f\\right ) \\\\ o_t&=\\sigma\\left ( W_ox_t+U_oh_{t-1}+V_oc_t+b_o\\right ) \\end{align} 耦合输入门和遗忘门 \\begin{align} c_t&=(1-i_t)\\odot c_{t-1} +i_t\\odot\\widetilde{c}_t \\end{align} GRU 不引入额外的记忆单元，而是引入更新门控制当前状态需要从历史状态中保留多少信息以及需要从候选状态中接受多少新信息。 GRU执行的步骤如下: 利用上一时刻的外部状态h_{t-1}和当前时刻的输入x_t计算出两个门（更新门、重置门）。 计算候选状态\\widetilde{h}_t，重置门用来控制候选状态\\widetilde{h}_t的计算依赖多少上一时刻的状态h_{t-1}。 使用更新门进行状态更新。 \\begin{align} z_t&=\\sigma\\left ( W_zx_t+U_zh_{t-1}+b_z\\right) \\\\ r_t&=\\sigma\\left ( W_rx_t+U_rh_{t-1}+b_r\\right) \\\\ \\widetilde{h}_t&=\\tanh\\left ( W_hx_t+U_h(r_t\\odot h_{t-1})+b_h \\right ) \\\\ h_t&=z_t\\odot h_{t-1} + (1-z_t)\\odot \\widetilde{h}_t \\end{align} 深层模型 堆叠循环神经网络 双向循环神经网络 应用框架 序列到类 同步的序列到序列 异步的序列到序列 "},"book/tutorial/ml/transformer.html":{"url":"book/tutorial/ml/transformer.html","title":"Transformer","keywords":"","body":" Transformer 常见问题 为什么使用“多头”注意力机制？ 为什么进行缩放\\sqrt{d_k}? 官方Transformer的multi-head attention有多少个“头”？q、k、v各有多少维？ Transformer的layer是由什么组成的？ Transformer完全没有使用循环网络结构，只使用了self-attention模块完成了seq2seq任务，这使得整个模型可以并行计算，不再受限于LSTM/GRU等模型的时序性质。 Multi-Head Attention Scaled Dot Product Attention ，将Query和Key-Value映射到输出 Attention \\left ( Q,K,V\\right )=Softmax(\\frac{QK^T}{\\sqrt{d_k}})V \\\\ head_i=Attention(QW_i^Q,KW_i^K,VW_i^V) \\\\ MultiHead(Q,K,V)=Concat(head_1,..,head_h)W^O \\\\ W_i^Q\\in \\mathbb{R}^{d_{model} \\times d_k},W_i^K\\in \\mathbb{R}^{d_{model} \\times d_k},W_i^V\\in \\mathbb{R}^{d_{model} \\times d_v},W_i^O\\in \\mathbb{R}^{h\\cdot d_v \\times d_{model}} import math import copy import torch import torch.nn as nn import torch.nn.functional as F def clones(module, N): \"Produce N identical layers.\" return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) def attention(query, key, value, mask=None, dropout=None): \"\"\" 输入形状: B*h*N*d_k | B*h*N*d_k | B*h*N*d_v | 1*1*N*N Batch、head是独立的，运算时为了方便思考可以不考虑 \"\"\" d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # B*h*N*N if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # B*h*N*N 在 0/1轴广播mask p_attn = F.softmax(scores, dim = -1) # Softmax不改变形状 B*h*N*N if dropout is not None: p_attn = dropout(p_attn) # dropout的概率结果归零，既不关注某个位置的单词 B*h*N*N return torch.matmul(p_attn, value), p_attn # B*h*N*N x B*h*N*d_v 每个单词与其他位置的单词V累加（矩阵乘） --> B*h*N*d_v class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): \"Take in model size and number of heads.\" super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h self.h = h self.linears = clones(nn.Linear(d_model, d_model), 4) self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): # 此处的输入已经假设是 Multi-Head 形式 # B*N*d_model | B*N*d_model | B*N*d_model | B*N*N if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) # 1*1*N*N nbatches = query.size(0) # 1) Do all the linear projections in batch from d_model => h x d_k # B*N*d_model x d_model*d_model => B*N*d_model==> B*N*h*d_k ==> B*h*N*d_k query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))] # 2) Apply attention on all the projected vectors in batch. # B*h*N*d_k --> B*h*N*d_v , B*h*N*N x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout) # 3) \"Concat\" using a view and apply a final linear. # B*h*N*d_v ==> B*N*h*d_v ==> B*N*d_model x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k) # B*N*d_model x d_model*d_model ==> B*N*d_model return self.linears[-1](x) 为什么进行缩放\\sqrt{d_k}? 对于较大的d_k，点积大幅增大, 将Softmax函数值推向具有极小梯度的区域 为了阐明点积变大的原因，假设q和k是独立的随机变量, 平均值为0，方差为1，这样他们的点积为q\\cdot k=\\sum_i q_ik_i，同样是均值0为方差为d_k。 为了抵消这种影响，我们用\\frac{1}{\\sqrt{d_k}}来缩放点积。 可以从余弦相似度的角度解释 cos(q,k) = \\frac{q\\cdot k}{\\parallel q\\parallel \\times \\parallel k\\parallel } 为什么使用“多头”注意力机制？ 把输入映射到不同的子空间，可以理解为从不同的角度观察，类似卷积核的思想。 Transformer的输入dim和Q/K/V及Head数h的关系？ d_q=d_k d_v和d_{model}可以任意维度 官方Transformer的multi-head attention有多少个“头”？q、k、v各有多少维？ 在Transformer中h=8 对每个Head有d_k=d_v=d_{model}/ 8=64 Transformer中哪些地方应用了 Multi-Head Attention机制？ Encoder中的Multi-Head Attention层。Key、Value和Query都来自前一层的输出，Encoder中当前层的每个位置都能Attend到前一层的所有位置。 Decoder中的Multi-Head Attention层（*）。Query来自先前的解码器层，Key和Value来自Encoder的输出，Decoder中的每个位置Attend输入序列中的所有位置。 Decoder中的Masted Multi-Head Attention层。Key、Value和Query都来自前一层的输出，Decoder中的每个位置Attend当前解码位置和它前面的所有位置。在缩放后的点积Attention logit中，屏蔽（设为负无穷）Softmax的输入中所有对应着非法连接的Value。 Position-Wise前馈网络 FFN(x)=max(0,xW_1+b_1)W_2+b_2 在Transformer中的Position-Wise前馈网络Layer中，包含两次线性变换，中间使用Relu激活函数，参数的维度为 W_1\\in \\mathbb{R}^{512 \\times 2048} , W_2\\in \\mathbb{R}^{2048 \\times 512} 每个P-W前馈网络对应一个位置（单词），不同的位置（单词）共享全连接参数。计算后得到的格式为 B\\times N\\times d_{model}，与输入格式一致。 class PositionwiseFeedForward(nn.Module): \"Implements FFN equation.\" def __init__(self, d_model, d_ff, dropout=0.1): super(PositionwiseFeedForward, self).__init__() self.w_1 = nn.Linear(d_model, d_ff) self.w_2 = nn.Linear(d_ff, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): # B*N*d_model x d_model*d_ff x d_ff*d_model ==> B*N*d_model return self.w_2(self.dropout(F.relu(self.w_1(x)))) Add & Norm LN(x)=\\alpha\\times \\frac{x-\\mu}{\\sigma + \\varepsilon}+\\beta 以上提到的Multi-Head Attention层和Position-Wise前馈网络都属于SubLayer，结合Add&Norm组成一个完整的Layer。需要注意的是，在该版本中Norm出现在每次进入sublayer时最开始的第一个操作。 class LayerNorm(nn.Module): \"Construct a layernorm module (See citation for details).\" def __init__(self, features, eps=1e-6): super(LayerNorm, self).__init__() self.a_2 = nn.Parameter(torch.ones(features)) self.b_2 = nn.Parameter(torch.zeros(features)) self.eps = eps def forward(self, x): # B*N*d_model ==> B*N*1 mean = x.mean(-1, keepdim=True) # B*N*d_model ==> B*N*1 std = x.std(-1, keepdim=True) # d_model * B*N*d_model + d_model ==> B*N*d_model return self.a_2 * (x - mean) / (std + self.eps) + self.b_2 class SublayerConnection(nn.Module): \"\"\" A residual connection followed by a layer norm. Note for code simplicity the norm is first as opposed to last. \"\"\" def __init__(self, size, dropout): super(SublayerConnection, self).__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): # B*N*d_model + B*N*d_model return x + self.dropout(sublayer(self.norm(x))) Layer Norm的作用是什么？ 随着网络深度的增加，数据的分布会不断发生变化,为了保证数据特征分布的稳定性，加入LN这样可以加速模型的收敛速度。 防止输入数据落在激活函数的饱和区，发生梯度消失的问题，使得模型训练变得困难。 Layer Norm 与 Batch Norm的区别？ BN的主要思想是: 在每一层的每一批数据(一个batch里的同一通道)上进行归一化。 LN的主要思想是:是在每一个样本(一个样本里的不同通道)上计算均值和方差，而不是 BN 那种在批方向计算均值和方差。 Encoder EncoderLayer 一个Layer包含两个Sublayer，分别是Attention Layer和 FFN Layer。 在Transformer中，Encoder包含6个Layer。 class EncoderLayer(nn.Module): \"Encoder is made up of self-attn and feed forward (defined below)\" def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): \"Follow Figure 1 (left) for connections.\" x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayer[1](x, self.feed_forward) class Encoder(nn.Module): \"Core encoder is a stack of N layers\" def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): \"Pass the input (and mask) through each layer in turn.\" for layer in self.layers: x = layer(x, mask) return self.norm(x) Decoder 除了拥有每个Encoder Layer中的两个Sublayer之外，Decoder还插入了第三种类型的Sublayer对Encoder的输出实行“多头”的Attention。 Mask确保了生成位置i的预测时，仅依赖小于i的位置处的已知输出，相当于把后面不该看到的信息屏蔽掉。 Mask Map def subsequent_mask(size): # 1*N*N attn_shape = (1, size, size) subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8') return torch.from_numpy(subsequent_mask) == 0 src-attn模块的输入来源于Encoder的Key、Value，Decoder上一层的self-attention的输出作为Query。 class DecoderLayer(nn.Module): \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\" def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): \"Follow Figure 1 (right) for connections.\" m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward) Positional Encoder Embedding PE_{pos,2i}=sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}) \\\\ PE_{pos,2i+1}=cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}) 其中pos是位置, i是维度。也就是说，位置编码的每个维度都对应于一个正弦曲线,。 在Transformer模型中，dropout=0.1。 在Encoder与Decoder的输入部分都加入了位置编码。 class PositionalEncoding(nn.Module): \"Implement the PE function.\" def __init__(self, d_model, dropout, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings once in log space. pe = torch.zeros(max_len, d_model) # 生成空位置向量 max_len*d_model position = torch.arange(0, max_len).unsqueeze(1) # max_len*1 div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) # 1*max_len*d_model self.register_buffer('pe', pe) # 设置为buffer（不需要求梯度的参数） def forward(self, x): # B*N*d_model + 1*N*d_model --> B*N*d_model x = x + torch.tensor(self.pe[:, :x.size(1)]) return self.dropout(x) 分类器 分类器使用一个Linear层+Softmax进行分类，置于Decoder之后。 class Generator(nn.Module): \"Define standard linear + softmax generation step.\" def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) def forward(self, x): # B*N*d_model x d_model*voc ==> B*N*voc return F.log_softmax(self.proj(x), dim=-1) Encoder-Decoder 通用的组织架构，同样适用于Transformer。 class EncoderDecoder(nn.Module): \"\"\" A standard Encoder-Decoder architecture. Base for this and many other models. \"\"\" def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): \"Take in and process masked src and target sequences.\" return self.decode(self.encode(src, src_mask), src_mask,tgt, tgt_mask) def encode(self, src, src_mask): return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1): \"Helper: Construct a model from hyperparameters.\" c = copy.deepcopy attn = MultiHeadedAttention(h, d_model) ff = PositionwiseFeedForward(d_model, d_ff, dropout) position = PositionalEncoding(d_model, dropout) model = EncoderDecoder( Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N), nn.Sequential(Embeddings(d_model, src_vocab), c(position)), nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), Generator(d_model, tgt_vocab)) # This was important from their code. # Initialize parameters with Glorot / fan_avg. for p in model.parameters(): if p.dim() > 1: nn.init.xavier_uniform(p) return model # Small example model. tmp_model = make_model(10, 10, 2) 训练 构造Batch和Mask class Batch: \"Object for holding a batch of data with mask during training.\" def __init__(self, src, trg=None, pad=0): # B*N self.src = src self.src_mask = (src != pad).unsqueeze(-2) # B*1*N if trg is not None: self.trg = trg[:, :-1] # B*(N-1) Decoder输入 self.trg_y = trg[:, 1:] # B*(N-1) Decoder输出 self.trg_mask = self.make_std_mask(self.trg, pad) self.ntokens = (self.trg_y != pad).data.sum() @staticmethod def make_std_mask(tgt, pad): \"Create a mask to hide padding and future words.\" tgt_mask = (tgt != pad).unsqueeze(-2) # B*1*N & 1*N*N ==> B*N*N tgt_mask = tgt_mask & torch.tensor(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)) return tgt_mask "},"book/tutorial/ml/pytorch.html":{"url":"book/tutorial/ml/pytorch.html","title":"PyTorch Tutorial","keywords":"","body":"PyTorch Tutorial 指导思想 为了建立清晰的框架，按照建模步骤进行学习不失为一个好的思路 得到一个有限的训练数据集合 确定包含所有可能的模型的假设空间，即学习模型的集合 确定模型选择的准则，即学习的策略（比如损失函数） 实现求解最优模型的算法，即学习的算法（比如梯度下降） 通过学习算法选择最优模型 利用学习的最优模型对新数据进行预测和分析 代码示例 import torch import torch.nn as nn from sklearn.datasets import load_boston boston = load_boston() class LinearModel(nn.Module): def __init__(self , ndim ): # 定义类的初始化参数, a,b,c是传入的参数 super(LinearModel,self).__init__() self.ndim = ndim self.weight = nn.Parameter(torch.randn(ndim,1)) self.bias = nn.Parameter(torch.randn(1)) def forward(self, x): # 定义前向计算的输入参数，一般是张量或其他的参数 return x.mm(self.weight) + self.bias lm = LinearModel(13) # 定义模型实例 criterion = nn.MSELoss() # 定义损失函数示例 optim = torch.optim.SGD(lm.parameters() , lr=1e-6) # 定义优化器实例 data = torch.tensor(boston[\"data\"],requires_grad=True, dtype=torch.float32) target = torch.tensor(boston[\"target\"][:,np.newaxis],dtype=torch.float32) for step in range(10000): predict = lm(data) # 模型预测结果 loss = criterion(predict , target) # 输出损失 if step and step % 1000 == 0 : print(\"Loss: {:.3f}\".format(loss.item())) optim.zero_grad() # 清零梯度 loss.backward() # 反向传播(梯度记录在对应张量里) optim.step() # 梯度下降 API基础 得到一个有限的训练数据集合 数据加载 DataSet import torch import seaborn as sns from torch.utils.data import Dataset class TitanicDataset(Dataset): def __init__(self): data = sns.load_dataset(\"titanic\") self.x_data = torch.from_numpy(data[['pclass','age','fare']].values) self.y_data = torch.from_numpy(data[['survived']].values) self.len = data.shape[0] def __getitem__(self, index): return self.x_data[index], self.y_data[index] def __len__(self): return self.len DataLoader 参数 注释 dataset torch.utils.data.DataSet 类的实例 batch_size=1 mini_batch的大小 shuffle=False 是否打乱数据,shuffle=True会构造默认的采样器进行采样 sampler=None 自定义采样器,构造 torch.utils.data.Sampler实例且设置shuffle=False batch_sampler=None 同上,返回batch数量的索引 num_workers=0 进程数目,默认单进程处理 collate_fn=None 定义如何把一批dataset转换为包含mini-batch数据的张量 pin_memory=False 转移数据到Pinned Memory(与GPU内存关联的CPU内存)，加快GPU载入数据速度 drop_last=False 如果最后一个batch小于设置数目,则丢弃 timeout=0 若设置大于0,表示最多等待的时间 worker_init_fn=None 数据载入的子进程开始时运行的函数 from torch.utils.data import DataLoader titanic = TitanicDataset() trainloader = DataLoader(titanic, batch_size=4,shuffle=True) 张量创建 torch.tensor 函数 列表 -> Tensor Numpy数组 -> Tensor 内置函数 rand [0,1)均匀分布 randn 标准正态分布\\mathbb{N}(0,1) randint(0,10,(3,3)) 整数均匀分布 zeros、ones、eye 形状拷贝 {内置函数}_like(t)mul 类型拷贝 t.new_tensor(列表/Numpy数组) t.new_{内置函数}(size) ，没有随机元素填充函数 存储设备 两个张量之间的运算只有在相同设备上才能进行 创建张量时指定 torch.randn(3, 3, device=\"cpu\") #默认 torch.randn(3, 3, device=\"cuda:0\") 获取张量所在设备 t.device 转移设备 t.cpu() t.cuda(1) 张量类型 数据并行化 通过学习算法选择最优模型 模型可视化 Tensorboard 在训练过程中，通过可视化能够直观地观测loss的趋势、张量的变化。 SummaryWriter 参数 说明 log_dir=None 指定文件存储目录 comment=\"\" 文件夹名称添加注释信息 purge_step 如果写入可视化数据崩溃,该步数之后的数据不再写入。数据写入是先进入队列,得到一定的数量触发文件的写入 max_queue 在写入磁盘之前内存中最多可保留的event(数据)的数量 flush_secs 多长时间写入一次 filename_suffix=\"\" 文件名前缀 # 安装 pip install tensorflow==1.14.0 tensorflow-tensorboard==1.5.1 tensorboard==1.14.0 # 启动TensorBoard 默认 6006 端口 tensorboard -logdir runs 可视化数据类型 add_scalars( tag , data , step ) Scalars 曲线变化趋势 一般格式服从Linux目录命名规则 name1/name2/... add_histogram( tag , data , step ) Distribution 最大最小值分布 Histogram 直方图分布 add_image( tag , data) Image 图片信息 默认是 CHW 通道,高度,宽度 一次传入多张图片 BCHW add_video( tag , data , fps ) 传入格式 NTCHW T为视频时间的方向 add_audio( tag , snd_tensor , sample_rate ) snd_tensor 是一个 1×L的张量,值在[-1,1]之间 sample_rate 采样频率 add_text( tag , text_string ) add_graph() 显示计算图 add_pr_curve(labels , predictions , num_thresholds , weights) 显示准确率-召回率曲线 调优经验 损失上升或者震荡,说明学习率选择偏大,可以尝试降低学习率 损失下降缓慢,说明学习率偏小,可以尝试适当增加学习率 由于每个mini-batch未必会使得损失下降,可通过Somoothing滑块平滑损失曲线,平均多个mini-batch 如果参数的分布一直没有变化,可能模型结构有问题或者梯度无法在模型中得到反向传播 6.利用学习的最优模型对新数据进行预测和分析 在模型选择和推断阶段，都需要建立checkpoint或者加载已有的模型数据,通过存储/加载模型结构和模型、优化器等参数达到想要的效果。 模型保存(序列化器) torch.save 参数 注释 obj 模型/张量/状态字典(推荐) f 文件路径 pickle_module=pickle 序列化库 pickle_protocol=2 pickle协议 0~4 模型加载(反序列化器) torch.load 参数 注释 f 文件路径 map_location=None 设置迁移环境后的CPU或GPU及名称映射 pickle_module=pickle 序列化库 **pickle_load_ars 其它参数传送到 pickle_module.load函数 状态字典 与pytorch版本关联性小,推荐以该形式保存 save_info = { \"iter_num\":iter_num, # 迭代步数 \"optimizer\": optimizer.state_dict(), # 优化器的状态字典 \"model\" : model.state_dict(), # 模型的状态字典 } # 保存信息 torch.save(save_info,save_path) # 载入信息 save_info = torch.load(save_path) optimizer.load_state_dict(save_info[\"optimizer\"]) model.load_state_dict(save_info[\"model\"]) NLP模块 sklearn N-gram词频特征 CountVectorizer TF-IDF特征 TfidfTransformer (基于CountVectorizer的结果) TfidfVectorizer \\text{TF_IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t) \\\\ \\text{IDF}(t) = \\log \\frac{1+N}{1+DF(t)}+1 word embedding层 nn.Embedding 词向量矩阵 查找表 实战篇 LSTM/Bi-LSTM分类 import numpy as np import torch import torch.nn as nn import torch.optim as optim def make_batch(): input_batch, target_batch = [], [] for seq in seq_data: input = [word_dict[n] for n in seq[:-1]] # 'm', 'a' , 'k' is input target = word_dict[seq[-1]] # 'e' is target input_batch.append(np.eye(n_class)[input]) # 造embedding向量 target_batch.append(target) return input_batch, target_batch class TextLSTM(nn.Module): \"\"\" LSTM + Linear分类器 \"\"\" def __init__(self): super(TextLSTM, self).__init__() self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden,batch_first=True) self.W = nn.Linear(n_hidden, n_class, bias=True) def forward(self, X): outputs, (_, _) = self.lstm(X) outputs = outputs[:,-1] # B*N*H --> B*H model = self.W(outputs) # model :B*H x H*O + O --> B*O return model class BiLSTM(nn.Module): def __init__(self): super(BiLSTM, self).__init__() # 因为 word embedding 是 one-hot编码，所以 input-size是 n_class self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden, bidirectional=True,batch_first=True) self.W = nn.Linear(n_hidden * 2, n_class, bias=True) def forward(self, X): outputs, (_, _) = self.lstm(X) outputs = outputs[:,-1] # B*(H+H) Concat 双向向量 model = self.W(outputs) # model : [batch_size, n_class] return model if __name__ == '__main__': n_hidden = 128 # number of hidden units in one cell char_arr = [c for c in 'abcdefghijklmnopqrstuvwxyz'] word_dict = {n: i for i, n in enumerate(char_arr)} number_dict = {i: w for i, w in enumerate(char_arr)} n_class = len(word_dict) # number of class(=number of vocab) seq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star'] model = TextLSTM() model2 = BiLSTM() criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model2.parameters(), lr=0.001) input_batch, target_batch = make_batch() input_batch = torch.FloatTensor(input_batch) target_batch = torch.LongTensor(target_batch) # Training for epoch in range(1000): optimizer.zero_grad() output = model2(input_batch) loss = criterion(output, target_batch) if (epoch + 1) % 100 == 0: print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss)) loss.backward() optimizer.step() inputs = [sen[:3] for sen in seq_data] predict = model2(input_batch).data.max(1)[1] print(inputs, '->', [number_dict[n.item()] for n in predict]) 源码分析 PyTorch模块组织结构 编号 模块 功能 说明 1 torch 激活函数、张量操作、张量创建 张量操作加下划线实现本地修改 2 torch.Tensor 张量类型 torch.Storage负责底层的数据存储 3 torch.sparse 稀疏张量 COO格式(Coordinate),长整型定义非零元素的位置，浮点数定义非零元素的值。稀疏张量之间可以做加减乘除运算和矩阵乘法。 4 torch.cuda CUDA相关函数 对GPU操作 5 torch.nn 神经网络模块、损失函数 继承nn.Module类重写forward方法实现新模块；该模块的内容一般都含有参数 6 torch.nn.functional 神经网络函数（操作）模块 另外还有不常用的激活函数 7 torch.nn.init 权重初始化 均匀初始化、正态分布归一化等 8 torch.optim 优化器 包括学习率衰减算法 9 torch.autograd 自动微分函数 还内置了数值梯度功能和检查自动微分引擎是否正确输出的功能 10 torch.distributed 分布式计算模块 支持后端MPI、Gloo、NCCL，原理是启动多个进程并行，最后对所有进程权重张量的梯度做规约，然后广播到各模型 11 torch.distributions 采样模块 常用于强化学习策略。策略梯度算法，规避离散变量不能求导的问题 12 torch.hub 预训练模型 list、load 13 torch.jit 即时编译器 动态图转静态图 14 torch.multiprocessing 多进程API 进程间共享张量（共享内存），每个进程运行不同模型 15 torch.random 保存和设置随机数生成器状态 设置一个统一的随机种子可以帮助测试不同结构神经网络的表现，方便调试 16 torch.onnx 开放神经网络交换格式（ONNX） 方便不同框架交换模型 17 torch.utils 辅助工具模块 bottleneck检查模块运行时间方便优化checkpoint记录计算过程节约内存cpp_extension定义C++扩展data数据集和载入器dlpack定义格式转换tensorboard可视化 张量操作 获取形状 t.size() t.shape 获取维度数目 t.ndimension() 获取总元素数目 t.nelement() 改变形状 t.view(3,4) t.view(-1,4) 自动推断 t.view(2,6).contiguous() 不兼容，生成新的张量(兼容:新张量的两个连续维度的乘积等于原来张量的某一维度) t.reshape(2,6) 相当于上面的语句 获得张量的数据指针 t.data_ptr 判断底层存储一致性 切片和索引 t[:,3:5,:] 共享内存 t[:,3:5,:].clone() t[t>0] 返回一维向量 张量运算 t.sqrt() t.sqrt_() In-Place操作 torch.sqrt(t) keepdim=True 在进行求积、求和、求平均计算的同时，会自动消除被计算的维度，该参数用于保留维度 张量的极值运算 t.argmax(t,1) 只返回位置 t.max(t,1) 返回元祖 ( value , idx) t.sort(-1) 排序 ，返回元祖 (value , idx) 张量乘法 torch.mm 矩阵乘法(线性变换) @运算符 同上 t.mm(t1) 同上 t.bmm(t1) 批次矩阵乘法 b×m×k @ b×k×n => b×m×n torch.einsum(\"bnk,bkl->bnl\" , t , t1 ) 爱因斯坦求和约定 张量的堆叠与拼接 torch.stack([t,t1]) 堆叠(增加维度) 3×4,3×4=>3×4×2 torch.cat([t,t1]) 拼接(维度不变) 3×4,3×4=>3×8 张量的分割与分块 t.split([1,3],-1) 分成 3×1和3×3两个张量 t.split(2,-1) 分成 最后一维大小为2的3×2大小的两个张量 t.chunk(2,-1) 分成 两个张量 (需要能整除) 张量维度的扩增与压缩 t.unsqueeze(-1) 扩维 3×4×1 t.squeeze() 压缩大小是1的维度 张量的广播 两个不同维度的张量不能做四则运算 两个张量对应维度需要至少有一个维度大小为1或者两个维度大小相等 模块类 获取模型参数 named_parameters parameters 返回生成器 模型参数初始化由初始化模型的类实例开始 nn.Parameters继承nn.Tensor ,默认 requires_grid=True 几乎所有张量创建方式都有requires_grid参数,一旦设置为True,后序中间结果的张量都会被设置成True 模型训练和测试状态转换 train eval 获取张量的缓存 named_buffers buffers 返回生成器 缓存同样是参数,但是不参与梯度传播，在训练中会得到更新,比如Batch-Norm 均值、方差参数 register_buffer 注册张量为缓存 获取模型的子模块 named_children children 返回生成器 named_modules modules 获取模块中所有模块的信息 apply函数应用 递归地对子模块应用自定义函数 模型参数类型、存储设备 lm.cuda() # 参数转移到GPU lm.half() # 参数转换为半精度浮点数 float16 计算图与自动求导 梯度与反向传播 t.grad_fn 导数函数 t.grad 存储梯度信息 t.grad.zero_() 梯度清零，默认多次反向传播梯度是累积的 torch.autograd.backward() 传入根节点张量和初始梯度张量（默认1） torch.autograd.grad(数据结果张量,需要对计算图求导的张量) 求梯度方法 计算图 retain_graph=True 反向传播时保留前向计算图 create_graph=True 创建梯度计算图 with torch.no_grad() 上下文管理器，不建立计算图，张量没有grid_fn t.detach() 与原计算图分离 损失函数与优化器 损失函数 模块形式 torch.nn 函数形式 torch.nn.functional loss.backward() 反向传播，记录梯度在对应张量里 torch.nn.BCELoss 二分类交叉熵损失 torch.nn.BECWithLogitsLoss 二分类几率交叉熵损失(直接输入几率,不需要Sigmoid) torch.nn.NLLLoss 负对数似然损失（先计算Softmax取对数的结果） torch.nn.functional.log_softmax torch.nn.CrossEntropyLoss 交叉熵损失(无需提前计算softmax取对数) 优化器 optim.zero_grad() 梯度清零 optim.step() 梯度下降 torch.optim.Adagrad( lr_decay , weight_decay , initial_accumulator_value) torch.optim.RMSprop( alpha , eps, weight_decay , momentum , centered) torch.optim.Adam( betas , eps, weight_decay , amsgrad) torch.optim.lr_scheduler.StepLR(optimizer , step_size=30 , gamma=0.1) 学习率衰减 torch.optim.lr_scheduler.CosineAnnealingLR 学习率衰减--余弦退火算法 "},"book/tutorial/ml/allennlp.html":{"url":"book/tutorial/ml/allennlp.html","title":"AllenNLP Tutorial","keywords":"","body":"AllenNLP Tutorial allennlp是基于PyTorch扩展的针对NLP任务的High-Level框架，用于提升建模效率及统一建模套路。 总体架构 graph LR A[DatasetReader]-->B[DataLoader] A-->E(Vocabulary) B-->D[Trainer] C[Model]-->D E-->C C-->F[Predictor] E-->F[Predictor] style A fill:#f96,stroke:#333,stroke-width:4px style C fill:#f96,stroke:#333,stroke-width:4px style D fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 style F fill:#bbf,stroke:#e86,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 一个NLP任务可以抽象成五个模块组成，其中DatasetReader模块和Model是需要用户参与建设的，allennlp提供了一部分内置组件可供用户使用，也可以根据需求自定义组件。 DatasetReader Embedder原则上属于Model部分,为了方便理解，在该部分一并介绍，Model部分不再介绍Embedder。 Method 继承DatasetReader实现自己的DatasetReader，尽量实现可重用的代码（比如按任务区分）,也可以为某个数据集定制。 重载_read()方法（必选） 重载 text_to_instance()方法 (可选) 最终返回 instance 实例 Code Guide import os from itertools import islice from typing import Dict, Iterable from allennlp.data import DatasetReader, Instance from allennlp.data.fields import LabelField, TextField from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer from allennlp.data.tokenizers import Token, Tokenizer, WhitespaceTokenizer # 继承 DatasetReader 并 _read() 和 text_to_instance() @DatasetReader.register('classification-tsv') class ClassificationTsvReader(DatasetReader): def __init__(self, tokenizer: Tokenizer = None, token_indexers: Dict[str, TokenIndexer] = None, max_tokens: int = None, **kwargs): super().__init__(**kwargs) self.tokenizer = tokenizer or WhitespaceTokenizer() self.token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()} self.max_tokens = max_tokens def text_to_instance(self, text: str, label: str = None) -> Instance: tokens = self.tokenizer.tokenize(text) if self.max_tokens: tokens = tokens[:self.max_tokens] text_field = TextField(tokens, self.token_indexers) fields = {'text': text_field} if label: fields['label'] = LabelField(label) return Instance(fields) def _read(self, file_path: str) -> Iterable[Instance]: with open(file_path, 'r') as lines: for line in lines: text, sentiment = line.strip().split('\\t') yield self.text_to_instance(text, sentiment) Text-to-Tensor Pipeline Text --> Token List --> Instance[Field(Indexer)] --> indexed tensor 一般步骤 选择合适的分词器，将str str str切分为List[str]， 同时使用Token类型维护每一个元素，得到List[Token]。 选择合适的Field维护每一个样本的X部分和Y部分,比如TextField[List[Token]]，同时指定索引器字典Dict[TokenIndexer]。 TextFieldEmbedder（可以看作embedders的盒子）包含不同的TokenEmbedder，分别embeds对应的索引空间的的索引序列，获得多组Embedding结果(如果指定了一个TokenEmbedder结果就一组)，默认情况下将多组Embedding向量在最后一维Concat起来（TextFieldEmbedder在发挥作用）。输入的张量shape一般是B\\times N,输出是B\\times N\\times D_{concat}，此时一个Token对应一个Embedding向量。 一般Token类型 字符 Characters (\"AllenNLP is great\" → [\"A\", \"l\", \"l\", \"e\", \"n\", \"N\", \"L\", \"P\", \" \", \"i\", \"s\", \" \", \"g\", \"r\", \"e\", \"a\", \"t\"]) 片段 Wordpieces (\"AllenNLP is great\" → [\"Allen\", \"##NL\", \"##P\", \"is\", \"great\"]) 单词 Words (\"AllenNLP is great\" → [\"AllenNLP\", \"is\", \"great\"]) 通用搭配方法 word-level tokenizer SingleIdTokenIndexer → Embedding (for things like GloVe or other simple embeddings, including learned POS tag embeddings) TokenCharactersIndexer → TokenCharactersEncoder (for things like a character CNN) ElmoTokenIndexer → ElmoTokenEmbedder (for ELMo) PretrainedTransformerMismatchedIndexer → PretrainedTransformerMismatchedEmbedder (for using a transformer like BERT when you really want to do modeling at the word level, e.g., for a tagging task) character-level tokenizer SingleIdTokenIndexer → Embedding wordpiece tokenizer PretrainedTransformerIndexer → PretrainedTransformerEmbedder SingleIdTokenIndexer → Embedding (if you don’t want contextualized wordpieces for some reason) Tips: 词表预留0作为padding，1作为unknow，正式单词从2开始索引 用代码来理解这个过程（5个例子） from allennlp.data import Vocabulary from allennlp.data.fields import TextField from allennlp.data.token_indexers import ( SingleIdTokenIndexer, TokenCharactersIndexer ) from allennlp.data.tokenizers import ( CharacterTokenizer, SpacyTokenizer, WhitespaceTokenizer, ) # 第一个例子，word分词+singleID索引 tokenizer = WhitespaceTokenizer() # 建立索引器时设置对应的词表空间，默认\"tokens\" token_indexer = SingleIdTokenIndexer(namespace='token_vocab') # 手动构造词表，指定词表空间 vocab = Vocabulary() vocab.add_tokens_to_namespace(['This', 'is', 'some', 'text', '.'], namespace='token_vocab') vocab.add_tokens_to_namespace(['T', 'h', 'i', 's', ' ', 'o', 'm', 'e', 't', 'x', '.'], namespace='character_vocab') # 构造一个样本 text = \"This is some text .\" tokens = tokenizer.tokenize(text) print(\"Word tokens:\", tokens) # 指定TextField时指定Indexer包，key作为索引空间。 # 索引空间和词表空间的区别：索引空间与Embedding层一一对应，不同的索引空间学习不同的Embedding矩阵。不同索引空间、同一词表空间表示可以共享词表，但不共享Embedding矩阵 text_field = TextField(tokens, {'tokens': token_indexer}) # 用词表生成Field对应的索引序列，结果维护在 indexed_tokens text_field.index(vocab) # 获得每个\"索引空间_key\"的长度 padding_lengths = text_field.get_padding_lengths() # 生成 indexed tensor 准备属入到Model tensor_dict = text_field.as_tensor(padding_lengths) print(\"With single id indexer:\", tensor_dict) # 第二个例子，，word分词+char索引, 使用另一个Indexer,指定词表空间 token_indexer = TokenCharactersIndexer(namespace='character_vocab') # 建立Field，维护在 索引空间 token_characters text_field = TextField(tokens, {'token_characters': token_indexer}) # 利用词表和索引器生成索引序列维护在Field ， 一个Token --> 一个索引序列 text_field.index(vocab) # 获得padding后的长度，后面用来构造tensor padding_lengths = text_field.get_padding_lengths() print(\"padding_lengths \", padding_lengths) # padding 并 转换 tensor tensor_dict = text_field.as_tensor(padding_lengths) # 生成Tensor格式 [B x N x C] print(\"With token characters indexer:\", tensor_dict) # 第三个例子：char 分词器 + SingleID索引器 # char 分词器(instead of words or wordpieces). tokenizer = CharacterTokenizer() tokens = tokenizer.tokenize(text) print(\"Character tokens:\", tokens) # 建立Single ID索引器，指定词表空间 token_indexer = SingleIdTokenIndexer(namespace='character_vocab') text_field = TextField(tokens, {'token_characters': token_indexer}) text_field.index(vocab) padding_lengths = text_field.get_padding_lengths() tensor_dict = text_field.as_tensor(padding_lengths) print(\"With single id indexer:\", tensor_dict) # 第四个例子，word分词+多个索引器 tokenizer = WhitespaceTokenizer() # word-->idx 和 word --> idx 序列 ， 指定词表空间 token_indexers = { 'tokens': SingleIdTokenIndexer(namespace='token_vocab'), 'token_characters': TokenCharactersIndexer(namespace='character_vocab') } text = \"This is some text .\" tokens = tokenizer.tokenize(text) print(\"Tokens:\", tokens) # 建立Field ， 配置 indexer字典 text_field = TextField(tokens, token_indexers) text_field.index(vocab) padding_lengths = text_field.get_padding_lengths() tensor_dict = text_field.as_tensor(padding_lengths) # 输出 索引空间+索引类型+Tensor的字典 Dict[str,Dict[str,Tensor]] print(\"Combined tensor dictionary:\", tensor_dict) # 第五个例子，使用Spacy库获得POS Tag，保存在Token.tag_ tokenizer = SpacyTokenizer(language=\"en_core_web_sm\", pos_tags=True) # 词表中添加 新的词表空间 vocab.add_tokens_to_namespace(['DT', 'VBZ', 'NN', '.'], namespace='pos_tag_vocab') # 建立三个索引器，分配索引空间和设置词表空间，其中两个single id索引器和一个char索引器 token_indexers = { 'tokens': SingleIdTokenIndexer(namespace='token_vocab'), 'token_characters': TokenCharactersIndexer(namespace='character_vocab'), 'pos_tags': SingleIdTokenIndexer(namespace='pos_tag_vocab', feature_name='tag_'), } tokens = tokenizer.tokenize(text) print(\"Token tags:\", [token.text for token in tokens], \"POS tags:\", [token.tag_ for token in tokens]) text_field = TextField(tokens, token_indexers) text_field.index(vocab) padding_lengths = text_field.get_padding_lengths() tensor_dict = text_field.as_tensor(padding_lengths) # 生成key分别是tokens、token_character、pos_tags索引空间 # 对应的值(内部字典)是key为tokens、token_characters、tokens索引类型的Tensor print(\"Tensor dict with POS tags:\", tensor_dict) Embedding indexed tensor --> Embedding Vector 常见方法 GloVe or word2vec embeddings Character CNNs POS tag embeddings Combination of GloVe and character CNNs wordpieces and BERT 注意事项 一个Embedder和一个TokenIndexer是一一对应的，使用索引空间标记。 输入Tensor里的一个索引值对应的一个Token经过Embedder操作输出时对应一个Embedding向量。 输入不同索引空间的Tensor里的多个索引值对应的是一个Token经过Embedder操作输出时也是对应一个Embedding向量。 默认使用BasicTextFieldEmbedder将多个索引空间的Embedding结果concat为一个Embedding向量。 用代码来理解这个过程 import torch from allennlp.modules.seq2vec_encoders import CnnEncoder from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder from allennlp.modules.token_embedders import ( Embedding, TokenCharactersEncoder ) from allennlp.data import Vocabulary import warnings warnings.filterwarnings(\"ignore\") ######### Demo1 # token_tensor 等价于 Model forward的输入 text ， 格式为 Dict[str,Dict[str,Tensor]] # Tensor的格式一般为为 [B x N] （word index） 或 [B x N x C] ( char index) # 第一个key表示索引空间名称（自定义），第二个key表示来源于Indexer的类型（自动生成） token_tensor = {'indexer1': {'tokens': torch.LongTensor([[1, 3, 2, 9, 4, 3]])}} # 配置框架方式会自动根据Vocab定义num_embeddings，只需要在jsonnet中定义embedding_dim embedding = Embedding(num_embeddings=10, embedding_dim=3) # 把定义好的 token embedders 放在 TextFieldEmbedder 里对应的索引空间中 embedder = BasicTextFieldEmbedder(token_embedders={'indexer1': embedding}) # 执行得到结果 ，格式为 B x N x D embedded_tokens = embedder(token_tensor) print(\"Using the TextFieldEmbedder:\", embedded_tokens) # 以上就是一般流程，如果就一个索引空间也可以不给它打包，直接使用 token embedder ， 这时候没有索引空间的概念 # 注意:在使用配置框架方法时需要按照一般流程进行 embedded_tokens = embedding(**token_tensor['indexer1']) print(\"Using the Embedding directly:\", embedded_tokens) ######### Demo2 # 这里是假设有两个索引空间 ，分别是 index1 [B x N] ， index2 [B x N x C] token_tensor = {'indexer2': {'token_characters': torch.tensor([[[1, 3, 0], [4, 2, 3], [1, 9, 5], [6, 0, 0]]])}, 'indexer1': {'tokens': torch.LongTensor([[1, 3, 2, 9]])}} # 定义一个 token embedder 对应 indexer1 索引空间 embedding = Embedding(num_embeddings=10, embedding_dim=3) # 定义另一个 token embedder 对应 indexer2 索引空间 character_embedding = Embedding(num_embeddings=10, embedding_dim=3) cnn_encoder = CnnEncoder(embedding_dim=3, num_filters=4, ngram_filter_sizes=(3,)) token_encoder = TokenCharactersEncoder(character_embedding, cnn_encoder) # 将两个 token_embedder 按照对应的索引空间打包 embedder = BasicTextFieldEmbedder(token_embedders={'indexer2': token_encoder, 'indexer1': embedding}) # 执行得到embedding结果 , 格式为 [B x N x D_concat] embedded_tokens = embedder(token_tensor) print(\"With a character CNN:\", embedded_tokens) ######### Demo3 # 再举一个例子，三个索引空间，使用了两个类型的Indexer token_tensor = { 'tokens': {'tokens': torch.LongTensor([[2, 4, 3, 5]])}, 'token_characters': {'token_characters': torch.LongTensor([[[2, 5, 3], [4, 0, 0], [2, 1, 4], [5, 4, 0]]])}, 'pos_tag_tokens': {'tokens': torch.tensor([[2, 5, 3, 4]])} } # 手动构造一个词表，指定词表空间。该过程由框架自动完成 vocab = Vocabulary() vocab.add_tokens_to_namespace(['This', 'is', 'some', 'text', '.'], namespace='token_vocab') vocab.add_tokens_to_namespace(['T', 'h', 'i', 's', ' ', 'o', 'm', 'e', 't', 'x', '.'], namespace='character_vocab') vocab.add_tokens_to_namespace(['DT', 'VBZ', 'NN', '.'], namespace='pos_tag_vocab') # 建立Embedding层时指定词表及词表空间，制定词表时无需指定 num_embeddings 参数(配置框架启动方式自动指定词表) embedding = Embedding(embedding_dim=3, vocab_namespace='token_vocab', vocab=vocab) character_embedding = Embedding(embedding_dim=4, vocab_namespace='character_vocab', vocab=vocab) cnn_encoder = CnnEncoder(embedding_dim=4, num_filters=5, ngram_filter_sizes=[3]) token_encoder = TokenCharactersEncoder(character_embedding, cnn_encoder) pos_tag_embedding = Embedding(embedding_dim=6, vocab_namespace='pos_tag_vocab', vocab=vocab) # 按照索引空间打包Token Embedder embedder = BasicTextFieldEmbedder( token_embedders={'tokens': embedding, 'token_characters': token_encoder, 'pos_tag_tokens': pos_tag_embedding}) embedded_tokens = embedder(token_tensor) print(embedded_tokens) Model 该部分是最具有灵活程度的，需要使用者具备一定的知识储备，可以选择完全自定义Model或利用内置Model构造自己的Model。 一般组成 Embedder Encoder Decoder 最简单的例子 from typing import Dict import torch from allennlp.data import Vocabulary from allennlp.models import Model from allennlp.modules import TextFieldEmbedder, Seq2VecEncoder from allennlp.nn import util from allennlp.training.metrics import CategoricalAccuracy # 配置框架注册名称 @Model.register('simple_classifier') class SimpleClassifier(Model): # 继承Model类 def __init__(self, vocab: Vocabulary, # 必选 embedder: TextFieldEmbedder, # 必选 encoder: Seq2VecEncoder): super().__init__(vocab) self.embedder = embedder self.encoder = encoder num_labels = vocab.get_vocab_size(\"labels\") self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels) # 线性分类器 self.accuracy = CategoricalAccuracy() def forward(self, text: Dict[str, torch.Tensor], label: torch.Tensor = None) -> Dict[str, torch.Tensor]: # Shape: (batch_size, num_tokens, embedding_dim) embedded_text = self.embedder(text) # Shape: (batch_size, num_tokens) mask = util.get_text_field_mask(text) # Shape: (batch_size, encoding_dim) encoded_text = self.encoder(embedded_text, mask) # Shape: (batch_size, num_labels) logits = self.classifier(encoded_text) # Shape: (batch_size, num_labels) probs = torch.nn.functional.softmax(logits, dim=-1) # Shape: (1,) output = {'probs': probs} if label is not None: self.accuracy(logits, label) output['loss'] = torch.nn.functional.cross_entropy(logits, label) return output def get_metrics(self, reset: bool = False) -> Dict[str, float]: return {\"accuracy\": self.accuracy.get_metric(reset)} 组成 Embedder 嵌入层 Encoder 编码器 Decoder 解码器（可选） Basic Classifier graph LR A[Embedder]-->B[Seq2Seq Encoder] B-->C[Seq2Vec Encoder] C-->D[FeedForward] D-->E(Cls Layer) A-->C C-->E Simple Tagger graph LR A[Embedder]-->B[Seq2Seq Encoder] B-->C(Tag Proj Layer) 附录 注册名称索引表 DatasetReader 名称 注册名称 来源 说明 ClsTsvDataSetReader cls_tsv_dataset_reader 自定义 按照Tab分隔的文本分类数据集 Token 分词器 名称 注册名称 来源 说明 空格分词器 whitespacejust_spaces 内置 默认选项,用于提前按空格整理好的数据 字符分词器 character 内置 按字符分词，默认区分大小写 Spacy分词器 spacy 内置 使用spacy库分词 字符数字分词器 letters_digits 内置 按照连续字符(汉字)、连续数字、空格分词 Transformer分词器 pretrained_transformer 内置 wordpieces单词片段 Jieba分词器 jieba 自定义 使用jieba库分词 Bert中文分词器 bert_token 自定义 # Token Indexer 索引器 可以组合多种索引形式以生成不同的embedding方式 名称 注册名称 来源 说明 唯一ID索引器 single_id 内置 映射一个token为唯一ID,默认0表示PADDING、1表示UNKNOW 字符索引器 characters 内置 以字符作为索引方式 Embedding 词嵌入模块 类型 类名称 注册名称 说明 TokenEmbedder Embedding embedding 一般推荐项 TokenEmbedder TokenCharactersEncoder character_encoding 组合Embedding+Seq2VecEncoder TextFieldEmbedder BasicTextFieldEmbedder basic 默认选项,TokenEmbedder封装器 Seq2Vec Encoder 名称 注册名称 来源 说明 BagOfEmbeddings boe 内置 最简单的seq2vec方法 RNN rnn 内置 继承pytorch seq2vec wrap类 LSTM lstm 内置 继承pytorch seq2vec wrap类 GRU gru 内置 继承pytorch seq2vec wrap类 Bi-LSTM stacked_bidirectional_lstm 内置 继承pytorch seq2vec wrap类 AlternatingLstm alternating_lstm 内置 继承pytorch seq2vec wrap类 1D CNN cnn 内置 Highway CNN cnn-highway 内置 [CLS] cls_pooler 内置 用于transformer训练 [CLS] bert_pooler 内置 用于bert训练 Seq2Seq Encoder 名称 注册名称 来源 说明 RNN rnn 内置 继承pytorch seq2seq wrap类 LSTM lstm 内置 继承pytorch seq2seq wrap类 GRU gru 内置 继承pytorch seq2seq wrap类 Bi-LSTM stacked_bidirectional_lstm 内置 继承pytorch seq2seq wrap类 AlternatingLstm alternating_lstm 内置 继承pytorch seq2seq wrap类 Transformer Encoder pytorch_transformer 内置 FFN feedforward 内置 组合模型 compose 内置 打包多个seq2seq encoder 门控CNN gated-cnn-encoder 内置 正处于测试阶段 占位符 pass_through 内置 空encoder,方便统一维护配置文件 Model 名称 注册名称 来源 任务类型 BasicClassifier basic_classifier 内置 分类 SimpleTagger simple_tagger 内置 标注 BiattentiveClassificationNetwork bcn 内置扩展 分类 CRFTagger crf_tagger 内置扩展 标注 LanguageModel language_model 内置扩展 语言模型 BidirectionalLanguageModel bidirectional-language-model 内置扩展 语言模型 Bert masked_language_model 内置扩展 语言模型 NextTokenLM next_token_lm 内置扩展 语言模型 Simple Seq2Seq simple_seq2seq 内置扩展 生成 CopyNet Seq2Seq copynet_seq2seq 内置扩展 生成 Composed Seq2Seq composed_seq2seq 内置扩展 生成 Bart bart_encoderbart 内置扩展 生成 Roberta transformer_mc 内置扩展 多任务 Trainer 名称 注册名称 来源 任务类型 Optimizer Optimizer|AdamOptimizer|adam|一般推荐项 实战演练 文本分类 1. "},"book/tutorial/ml/snorkel.html":{"url":"book/tutorial/ml/snorkel.html","title":"Snorkel Tutorial","keywords":"","body":" Snorkel Snorkel is a system for programmatically building and managing training datasets without manual labeling. 弱监督学习 类型 策略 仅部分labeled data active learning 主动学习、semi-supervised learning 半监督学习 label粒度较粗 multi-instance learning 多示例学习 存在错误或模棱两可的label learning with label noise 带噪学习 snorkel一般步骤 标注函数 (LFs) 方法 ① 模式匹配 （关键字、正则匹配） ② 启发式规则 （句子长度） ③ 第三方模型 （词性、其它近似任务） ④ 远程监督 ⑤ 众包标注 步骤 观测样本或利用先验知识总结模式 写一个LF初始版本 检查在采样的训练集（或验证集）上的效果 循环步骤不断改进LFs，提升覆盖率/准确率 from snorkel.labeling import labeling_function @labeling_function() def lf_keyword_my(x): \"\"\"Many spam comments talk about 'my channel', 'my video', etc.\"\"\" return SPAM if \"my\" in x.text.lower() else ABSTAIN 建模 & 融合 LFs label matrix, L_train ==> 行表示data point，列表示LFs evaluate metrics Polarity 标签集合 Coverage 覆盖率 Overlaps 重叠率 至少存在一个其它LF支持该LF的选择 Conflicts 冲突率 至少存在一个其它LF不同于该LF的选择 Correct 正确数 (存在gold labels) Incorrect 错误数 (存在gold labels) Accuracy 准确率 (存在gold labels) # evaluate LFs LFAnalysis(L=L_train, lfs=lfs).lf_summary() # 方便过滤指定label进行分析 from snorkel.analysis import get_label_buckets buckets = get_label_buckets(L_train[:, 0], L_train[:, 1]) df_train.iloc[buckets[(ABSTAIN, SPAM)]].sample(10, random_state=1) LabelModel ==> modeling LFs , 类似 stacking 的思路 from snorkel.labeling.model import LabelModel from snorkel.labeling import PandasLFApplier , LFAnalysis # Define the set of labeling functions (LFs) lfs = [lf_keyword_my, lf_regex_check_out, lf_short_comment, lf_textblob_polarity] # Apply the LFs to the unlabeled training data applier = PandasLFApplier(lfs) L_train = applier.apply(df_train) # Train the label model and compute the training labels label_model = LabelModel(cardinality=2, verbose=True) label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123) df_train[\"label\"] = label_model.predict(L=L_train, tie_break_policy=\"abstain\") 删除 ABSTAIN 数据形成 Train Data df_train = df_train[df_train.label != ABSTAIN] 变换函数 (TFs) for Data Augmentation 策略: text随机替换同义词 切片函数 (SFs) for Data Subset Selection 训练最终模型,提升泛化能力 "},"book/tutorial/ml/metric.html":{"url":"book/tutorial/ml/metric.html","title":"Metric","keywords":"","body":"评价指标 分类 混淆矩阵 - 预测值=1 预测值=0 真实值=1 TP 真阳性 FN 假阴性 真实值=0 FP 假阳性 TN 真阴性 准确率 Accuracy Accuracy = \\frac{TP + TN}{TP+FP+TN+FN} 精确率 、查准率 Precision Precision = \\frac{TP}{TP+FP} 召回率、查全率 Recall Recall = \\frac{TP}{TP+FN} F1 Score 调和平均数 \\begin{align} F1&=\\frac{1}{\\frac{1}{2}\\cdot \\frac{1}{P}+\\frac{1}{2}\\cdot \\frac{1}{R}} \\\\ &=\\frac{2\\cdot P\\cdot R}{P+R} \\end{align} Fβ Score \\begin{align} F\\beta&=\\frac{1}{\\frac{1}{\\beta+1}\\cdot \\frac{1}{P}+\\frac{\\beta}{\\beta+1}\\cdot \\frac{1}{R}} \\\\ &=\\frac{(1+\\beta)\\cdot P\\cdot R}{\\beta P+R} \\end{align} P-R曲线 ROC曲线、AUC "},"book/tutorial/ml/optimizer.html":{"url":"book/tutorial/ml/optimizer.html","title":"Optimizer","keywords":"","body":" Optimizer 参考文章 finding the parameters \\theta of a neural network that significantly reduce a cost function J(\\theta) , which typically includes a performance measure evaluated on the entire training set as well as additional regularization terms 1、 梯度下降法 Gradient Descent \\theta_{t+1}=\\theta_t - \\alpha \\cdot \\triangledown J(\\theta) 特点 学习率不好选择，过低收敛缓慢，过高波动太大 所有参数使用同样的学习率 容易收敛到局部最优，可能会被困在鞍点（梯度为0） Batch Gradient Descent (BGD) n表示所有样本数量 \\theta_{t+1}=\\theta_t - \\alpha \\cdot \\frac{1}{n}\\sum_{i=1}^{n} \\triangledown J_i(\\theta,x^i,y^i) Stochastic Gradient Descent (SGD) 每一个step仅一个样本，现实中一般指mini-batch \\theta_{t+1}=\\theta_t - \\alpha \\cdot \\triangledown J_i(\\theta,x^i,y^i) Mini-Batch Gradient Descent m表示一个batch size \\theta_{t+1}=\\theta_t - \\alpha \\cdot \\frac{1}{m}\\sum_{i=1}^{m}\\triangledown J_i(\\theta,x^i,y^i) 2、动量法 \\mu 动量因子（衰减系数），表示保持的动量比重，通常取0.9 m_t 指t时刻的动量 特点 加快收敛并且减少动荡 Momentum \\begin{align} m_{t+1} &= \\mu \\cdot m_t + \\alpha \\cdot \\triangledown J(\\theta) \\\\ \\theta_{t+1} &= \\theta_t - m_{t+1} \\end{align} NAG（Nesterov accelerated gradient） 在原始形式中，NAG相对于Momentum的改进在于，以“向前看”看到的梯度而不是当前位置梯度去更新。经过变换之后的等效形式中，NAG算法相对于Momentum多了一个本次梯度相对上次梯度的变化量，这个变化量本质上是对目标函数二阶导的近似。由于利用了二阶导的信息，NAG算法才会比Momentum具有更快的收敛速度 \\begin{align} m_{t+1} &= \\mu \\cdot m_t + \\alpha \\cdot \\triangledown J(\\theta - \\mu \\cdot m_t) \\\\ \\theta_{t+1} &= \\theta_t - m_{t+1} \\end{align} 3、自适应学习率算法 n_t 梯度累计变量 \\varepsilon 极小常量值防止分母为0，一般设置 10e-8 Adagrad \\delta 全局学习率，一般设置 0.01 特点 作为约束项\\frac{1}{\\sqrt{\\sum_{i=0}^{t}g_i^2 +\\varepsilon }}，随着梯度累计越来越多，学习率被衰减的越多，中后期参数更新量可能趋近于0，无法学习。 仍然需要设置全局学习率\\delta，如果设置过大会使约束项过于敏感，对梯度调节太大。 \\begin{align} g &= \\triangledown_{\\theta} J(\\theta) \\\\ n_t &= n_{t-1} + g^2 \\\\ \\triangle \\theta &= \\frac{\\delta}{\\sqrt{n_t + \\varepsilon}}\\cdot g \\\\ \\theta &= \\theta - \\triangle \\theta \\end{align} RMSprop \\delta 全局学习率，一般设置 0.001 \\nu 衰减系数，一般设置 0.9 特点 修改了AdaGrad的梯度平方和累加为指数加权的移动平均，使得其在非凸设定下效果更好。 RMSprop依然依赖于全局学习率\\rho 适合处理非平稳目标(包括季节性和周期性)——对于RNN效果很好 \\begin{align} g &= \\triangledown_{\\theta} J(\\theta) \\\\ n_t &= \\nu \\cdot n_{t-1} + (1-\\nu) \\cdot g^2 \\\\ \\triangle\\theta &= \\frac{\\delta}{\\sqrt{n_t + \\varepsilon}}\\cdot g \\\\ \\theta &= \\theta - \\triangle \\theta \\end{align} Adam (AdamW , HuggingfaceAdamW) \\delta 全局学习率，一般设置 0.001 \\mu 控制1阶动量，一般设置 0.9 \\nu 控制2阶动量，一般设置 0.999 w AdamW参数 - weight decay ，一般设置 0.01 特点 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点 自动调整参数的学习率 也适用于大多非凸优化 - 适用于大数据集和高维空间 AdamW相当于在原Loss的基础上添加了L2 Norm HuggingFace-AdamW有一点区别，w系数后的参数使用的未添加L2 Norm的\\theta_{t+1} \\begin{align} g &= \\triangledown_{\\theta} J(\\theta) \\\\ m_t &= \\mu \\cdot m_{t-1} + (1-\\mu) \\cdot g \\\\ n_t &= \\nu \\cdot n_{t-1} + (1-\\nu) \\cdot g^2 \\\\ \\hat{m_t} &= \\frac{m_t}{1-\\mu^t} \\\\ \\hat{n_t} &= \\frac{n_t}{1-\\nu^t} \\\\ \\triangle \\theta &= \\frac{\\delta}{\\sqrt{\\hat{n_t} +\\varepsilon}}\\cdot \\hat{m_t} \\\\ \\triangle \\theta &= \\delta \\cdot (\\frac{1}{\\sqrt{\\hat{n_t} + \\varepsilon}}\\cdot \\hat{m_t} + w\\theta_t )【AdamW】\\\\ \\theta_{t+1} &= \\theta_t - \\triangle \\theta \\end{align} 更多方法 lookahead 训练过程中改变学习率的方法 noam lr(s)=\\frac{\\lambda}{\\sqrt{d_m}}\\cdot min(\\frac{1}{s^{0.5}},\\frac{s}{s_{warmup}^{1.5}}) Optuna : A hyperparameter optimization framework 概念 Trial 一次针对目标函数的尝试 Study 包含多次尝试的一次会话 ，指定study_name 优化方向direction ; 指定sampler/pruner ; 设置db级别的storage用于并行(同时启动多个任务即可) Parameter 需要优化的参数 常用API 执行study study.optimize(objective, n_trials=100) 获得study后的最优parameter study.best_params 获得study后的最小目标函数值 study.best_value 获得study后的最佳trial的信息 study.best_trial 获得study后的所有trials信息 study.trials 定义参数空间 分支 if / 循环 for categorical / int / float optimizer = trial.suggest_categorical(\"optimizer\", [\"MomentumSGD\", \"Adam\"]) num_channels = trial.suggest_int(\"num_channels\", 32, 512, log=True) drop_path_rate = trial.suggest_float(\"drop_path_rate\", 0.0, 1.0, step=0.1) 优化算法 sampling Algorithms (缩减参数空间规模） Tree-structured Parzen Estimator algorithm optuna.samplers.TPESampler (默认) CMA-ES based algorithm optuna.samplers.CmaEsSampler Grid Search optuna.samplers.GridSampler Random Search optuna.samplers.RandomSampler pruning Algorithms (自动化的 early-stopping)) Asynchronous Successive Halving algorithm optuna.pruners.SuccessiveHalvingPruner Hyperband algorithm optuna.pruners.HyperbandPruner Median pruning algorithm optuna.pruners.MedianPruner (默认) Prune if the trial’s best intermediate result is worse than median of intermediate results of previous trials at the same step. Threshold pruning algorithm optuna.pruners.ThresholdPruner 指定固定的 max/min threashold 作为判断标准 应用指定方法进行裁剪 Trial.report / Trial.should_prune 搭配建议 RandomSampler + MedianPruner / TPESampler + Hyperband ( for not deep-learning) 通过 callback 的方式与其他框架整合 optuna.integration.* sklearn + optuna + pruner import optuna import sklearn.datasets import sklearn.linear_model import sklearn.model_selection def objective(trial): iris = sklearn.datasets.load_iris() classes = list(set(iris.target)) train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split( iris.data, iris.target, test_size=0.25, random_state=0 ) alpha = trial.suggest_float(\"alpha\", 1e-5, 1e-1, log=True) clf = sklearn.linear_model.SGDClassifier(alpha=alpha) for step in range(100): clf.partial_fit(train_x, train_y, classes=classes) # Report intermediate objective value. intermediate_value = 1.0 - clf.score(valid_x, valid_y) trial.report(intermediate_value, step) # report 汇报每个step的验证集评价 # Handle pruning based on the intermediate value. if trial.should_prune(): raise optuna.TrialPruned() return 1.0 - clf.score(valid_x, valid_y) # 返回最终的验证集评价 study = optuna.create_study(pruner=optuna.pruners.MedianPruner()) study.optimize(objective, n_trials=20) allennlp + optuna + pruner // alennlp hparams.json [ { \"type\": \"int\", \"attributes\": { \"name\": \"embedding_dim\", \"low\": 64, \"high\": 128 } }, { \"type\": \"float\", \"attributes\": { \"name\": \"dropout\", \"low\": 0.0, \"high\": 0.5 } }, { \"type\": \"categorical\", \"attributes\": { \"name\": \"kernel\", \"choices\": [\"linear\", \"poly\", \"rbf\"] } } ] // optuna.json { \"pruner\": { \"type\": \"HyperbandPruner\", \"attributes\": { \"min_resource\": 1, \"reduction_factor\": 5 } }, \"sampler\": { \"type\": \"TPESampler\", \"attributes\": { \"n_startup_trials\": 5 } } } // *.jsonnet local embedding_dim = std.parseInt(std.extVar('embedding_dim')); local dropout = std.parseJson(std.extVar('dropout')); local kernel = std.parseJson(std.extVar('kernel')); trainer: { // NOTE add `optuna_pruner` here! epoch_callbacks: [ { type: 'optuna_pruner', } ], num_epochs: num_epochs, optimizer: { lr: lr, type: 'sgd', }, validation_metric: '+accuracy', } # 训练 allennlp tune \\ imdb_optuna.jsonnet \\ hparams.json \\ --optuna-param-path optuna.json \\ --serialization-dir result \\ --study-name demo \\ --direction maximize \\ --storage mysql://:@/ \\ --skip-if-exists # 获得参数结果 allennlp best-params \\ --study-name demo \\ --storage mysql://:@/ # 使用最佳参数 Retrain allennlp retrain \\ imdb_optuna.jsonnet \\ --serialization-dir retrain_result \\ --study-name demo \\ --storage mysql://:@/ "},"book/tutorial/ml/loss.html":{"url":"book/tutorial/ml/loss.html","title":"Loss","keywords":"","body":" 损失函数 facal loss \\gamma 一般设置 2:负责降低简单样本的损失值, 以解决加和后负样本loss值很大 \\alpha_t 一般设置 0.25:调和正负样本的不平均，如果设置0.25, 那么就表示负样本为0.75, 对应公式 1-\\alpha FL(p_t)=-\\alpha_t(1-p_t)^\\gamma log(p_t) "},"book/papers/GlobalPointer.html":{"url":"book/papers/GlobalPointer.html","title":"GlobalPointer","keywords":"","body":"GlobalPointer GlobalPointer用统一的方式处理嵌套和非嵌套NER，可以达到和CRF一样甚至更好的效果。 在训练过程中，不需要像CRF那样递归计算分母，因此训练比CRF快很多。 在预测过程中，不需要像CRF那样使用动态规划进行解码，因此速度也会比CRF快。 总结来说，GlobalPointer设计更优雅，更快速，更强大。 一、基本思路 一般的pointer网络，会将首和尾分开，各用一个模块去识别，后续还需要使用二分类模型去补救，而GlobalPointer的Global体现在：它将实体的首尾位置视为一个整体去识别。 核心思想：将序列标注问题转化为m个\"\\frac{n(n+1)}{2}选k\"的多标签分类问题，以实体为基本单位进行识别。(CRF的复杂度为m^n) 二、数学形式 输入： [\\pmb{h}_1, \\pmb{h}_2, ...... ,\\pmb{h}_n] 变换： \\begin{align} \\pmb{q}_{i,\\alpha}=\\pmb{W}_{q,\\alpha}\\pmb{h}_i+\\pmb{b}_{q,\\alpha} \\Rightarrow [\\pmb{q}_{1,\\alpha}, \\pmb{q}_{2,\\alpha}, ...... , \\pmb{q}_{n,\\alpha}] \\\\ \\pmb{k}_{i,\\alpha}=\\pmb{W}_{k,\\alpha}\\pmb{h}_i+\\pmb{b}_{k,\\alpha} \\Rightarrow [\\pmb{k}_{1,\\alpha}, \\pmb{k}_{2,\\alpha}, ...... , \\pmb{k}_{n,\\alpha}] \\end{align} 那么，从i到j的连续片段是一个类型为\\alpha的实体的打分函数为： s_\\alpha(i, j)=\\pmb{q}_{i,\\alpha}^T\\pmb{k}_{j,\\alpha} 即Multi-Head Attention的简化版。 三、融入相对位置信息 如果没有相对位置信息输入的话，GlobalPointer对实体的长度和跨度都不是特别敏感，因此很容易把任意两个实体的首尾组合当成目标预测出来。 举例：北京：21度；上海：22度；杭州：23度；广州：24度； 预测结果：北京：21度；上海（地点实体） 融入旋转式相对位置编码(RoPE): \\begin{aligned} s_\\alpha(i,j)&=(\\pmb{R}_i\\pmb{q}_{i,\\alpha})^T(\\pmb{R}_j\\pmb{k}_{j,\\alpha}) \\\\ &=\\pmb{q}_{i,\\alpha}^T\\pmb{R}_i^T\\pmb{R}_j\\pmb{k}_{j,\\alpha} \\\\ &=\\pmb{q}_{i,\\alpha}^T\\pmb{R}_{i-j}\\pmb{k}_{j,\\alpha} \\end{aligned} RoPE编码方式来自于一篇论文：《RoFormer: Enhanced Transformer with Rotary Position Embedding》，其亮点在于用绝对位置编码方式实现相对位置编码，稍后会细讲。 四、损失函数 对于打分函数s_\\alpha(i,j)设计损失函数，最简单的想法是将多标签分类转换为\\frac{n(n+1)}{2}个二分类问题，将多个二分类的交叉熵损失相加。但一般n不小，\\frac{n(n+1)}{2}就更大，但实体数量不会很多，如果是\\frac{n(n+1)}{2}个二分类的话，会带来严重的类别不均衡问题。可以考虑将“softmax交叉熵”推广到多标签分类问题： log(1+\\sum_{(i,j)\\in P_\\alpha}e^{-s_\\alpha(i,j)}) + log(1+\\sum_{(i,j)\\in Q_\\alpha}e^{s_\\alpha(i,j)}) 其中， \\begin{aligned} \\Omega&=\\{(i, j) | 1 \\leq i \\leq j \\leq n\\} \\\\ P_\\alpha &= \\{(i, j) | t[i:j]是类型为\\alpha的实体\\} \\\\ Q_\\alpha &= \\Omega - P_\\alpha \\end{aligned} 在解码阶段，所有满足s_\\alpha(i,j)>0的片段t[i:j]都被视为\\alpha的实体输出，解码过程非常简单。稍后会细讲该损失函数。 "},"book/papers/RoPE.html":{"url":"book/papers/RoPE.html","title":"RoPE","keywords":"","body":"RoPE 绝对位置编码优点：实现简单，计算速度快。 相对位置编码优点：直接体现了相对位置信号，实际性能更好。 如果能够使用绝对位置编码的方式实现相对位置编码，那便“鱼和熊掌兼得”。 RoPE \\Rightarrow Rotrary Position Embedding 一、问题抽象 如果想要用绝对位置编码的方式实现相对位置编码。 （1）那么我们先要考虑，假设能够设计出操作\\pmb{f}(\\cdot, m)，\\pmb{f}(\\cdot, n)，使得经过该操作后，\\widetilde{\\pmb{q}}_m，\\widetilde{\\pmb{k}}_n就带有了m，n的绝对位置信息，即： \\begin{aligned} \\widetilde{\\pmb{q}}_m &= \\pmb{f}(\\pmb{q}, m) \\\\ \\widetilde{\\pmb{k}}_n &= \\pmb{f}(\\pmb{k}, n) \\end{aligned} （2）下一步需要考虑，Attention的核心运算是内积，所以我们希望内积运算结果可以自动带有相对位置信息，因此假设存在恒等关系： = g(\\pmb{q}, \\pmb{k}, m-n) 如果我们能够根据恒等式，求出\\pmb{f}(\\cdot, m)一个尽可能简单的解，那么我们就能够得到满足以上两个条件的变换操作，即用绝对位置编码的方式实现了相对位置编码。 设初始条件：\\pmb{f}(\\pmb{q},0) = \\pmb{q}，\\pmb{f}(\\pmb{k},0) = \\pmb{k} 二、求解 借助复数来求解。 因为在复数中有 = Re[\\pmb{q}\\pmb{k}^*]，再根据恒等式我们可以得到: Re[\\pmb{f}(\\pmb{q}, m)\\pmb{f}^*(\\pmb{k}, n)] = g(\\pmb{q},\\pmb{k},m-n) \\tag{1.1} 若想让(1)式成立的话，我们假设存在复数\\pmb{g}(\\pmb{q}, \\pmb{k}, m-n)，使得 \\pmb{f}(\\pmb{q}, m)\\pmb{f}^*(\\pmb{k}, n)=\\pmb{g}(\\pmb{q}, \\pmb{k}, m-n) \\tag{1.2} 使用复数的指数形式： \\begin{aligned} \\pmb{f}(\\pmb{q}, m) &= R_{\\pmb{f}}(\\pmb{q},m)e^{i\\theta_{\\pmb{f}}(\\pmb{q},m)} \\\\ \\pmb{f}(\\pmb{k}, n) &= R_{\\pmb{f}}(\\pmb{k},n)e^{i\\theta_{\\pmb{f}}(\\pmb{k},n)} \\\\ \\pmb{g}(\\pmb{q}, \\pmb{k}, m-n) &= R_{\\pmb{g}}(\\pmb{q},\\pmb{k},m-n)e^{i\\theta_{\\pmb{g}}(\\pmb{q},\\pmb{k},m-n)} \\end{aligned} \\tag{2} 如果我们能够求出(2)式中的R和\\theta，那么我们就可以表示变换f。 将(2)式代入(1.2)式得： \\begin{aligned} R_{\\pmb{f}}(\\pmb{q},m)\\cdot R_{\\pmb{f}}(\\pmb{k},n) &= R_{\\pmb{g}}(\\pmb{q},\\pmb{k},m-n) \\\\ \\theta_{\\pmb{f}}(\\pmb{q},m)-\\theta_{\\pmb{f}}(\\pmb{k},n) &= \\theta_{\\pmb{g}}(\\pmb{q},\\pmb{k},m-n) \\end{aligned} \\tag{3} 对(3)式中的第一个等式代入m=n=0: \\begin{aligned} R_{\\pmb{f}(\\pmb{q},m)}\\cdot R_{\\pmb{f}(\\pmb{k},m)} &= R_{\\pmb{g}(\\pmb{q},\\pmb{k},0)} \\\\ &= R_{\\pmb{f}(\\pmb{q},0)}\\cdot R_{\\pmb{f}(\\pmb{k},0)} \\\\ & = ||\\pmb{q}||\\cdot||\\pmb{k}|| \\end{aligned} \\tag{4} \\Rightarrow \\begin{aligned} R_{\\pmb{f}}(\\pmb{q},m) &= ||\\pmb{q}|| \\\\ R_{\\pmb{f}}(\\pmb{k},m) &= ||\\pmb{k}|| \\end{aligned} \\tag{7} 即不依赖于m。 对(3)式中的第二个等式代入m=n=0: \\begin{aligned} \\theta_{\\pmb{f}}(\\pmb{q},m)-\\theta_{\\pmb{f}}(\\pmb{k},m) &= \\theta_{\\pmb{g}}(\\pmb{q},\\pmb{k},0) \\\\ &= \\theta_{\\pmb{f}}(\\pmb{q},0)-\\theta_{\\pmb{f}}(\\pmb{k},0) \\\\ &= \\theta(\\pmb{q})-\\theta(\\pmb{k}) \\end{aligned} \\tag{5} \\Rightarrow \\theta_{\\pmb{f}}(\\pmb{q},m)-\\theta(\\pmb{q})=\\theta_{\\pmb{f}}(\\pmb{k},m)-\\theta(\\pmb{k}) 可以看出\\theta_{\\pmb{f}}(\\pmb{q},m)-\\theta(\\pmb{q})只与m相关，记为\\psi(m)，则有: \\theta_{\\pmb{f}}(\\pmb{q},m)=\\theta(\\pmb{q})+\\psi(m) \\tag{6} 令n=m-1: \\begin{aligned} \\psi(m) &= \\theta_{\\pmb{f}}(\\pmb{q},m) - \\theta(\\pmb{q}) \\\\ \\psi(m-1) &= \\theta_{\\pmb{f}}(\\pmb{q},m-1) - \\theta(\\pmb{q}) \\end{aligned} \\Rightarrow \\begin{aligned} \\psi(m) - \\psi(n) &= \\psi(m) - \\psi(m-1) \\\\ &= \\theta_{\\pmb{f}}(\\pmb{q},m) - \\theta_{\\pmb{f}}(\\pmb{q},m-1) \\end{aligned} 将(3)式中的第二行代入： \\begin{aligned} \\psi(m) - \\psi(m-1) &= \\theta_{\\pmb{g}}(\\pmb{q},\\pmb{k},m-n)+\\theta_{\\pmb{f}}(\\pmb{k},n)-\\theta_{\\pmb{f}}(\\pmb{q},m-1) \\\\ &= \\theta_{\\pmb{g}}(\\pmb{q},\\pmb{k},1) + \\theta_{\\pmb{f}}(\\pmb{k},m-1) - \\theta_{\\pmb{f}}(\\pmb{q},m-1) \\end{aligned} 将(5)式代入： \\begin{aligned} \\psi(m) - \\psi(m-1) &= \\theta_{\\pmb{g}}(\\pmb{q},\\pmb{k},1) + \\theta(\\pmb{k}) - \\theta(\\pmb{q}) \\end{aligned} 可以看出\\psi(m) - \\psi(m-1)与m无关，因此\\psi(m)是等差数列。设\\psi(m) - \\psi(m-1) = \\theta，\\theta为常数。根据等差数列通项公式，有： \\begin{aligned} \\psi(m) &= \\psi(m = 0) + m\\theta \\\\ &= 0 + m\\theta \\\\ \\Rightarrow \\psi(m) &= m\\theta \\end{aligned} \\tag{8} 由此我们推得了变换\\pmb{f}(\\pmb{q}, m): \\begin{aligned} \\pmb{f}(\\pmb{q}, m) &= R_{\\pmb{f}}(\\pmb{q},m)e^{i\\theta_{\\pmb{f}}(\\pmb{q},m)} \\\\ 根据(6),(7),(8)\\Rightarrow &= ||\\pmb{q}||e^{i(\\theta(\\pmb{q})+m\\theta)} \\\\ &= ||q||e^{i\\theta(\\pmb{q})}\\cdot e^{im\\theta} \\\\ &= \\pmb{q}\\cdot e^{im\\theta} \\end{aligned} \\tag{9} 得到变换的结果后，是无法直接写成代码实现的。但我们可以发现该变换是两个复数相乘，两个复数相乘的几何意义为旋转。 三、转换成代码形式 根据(9)式以及复数相乘的几何意义，我们可以理解成(9)式就是对复数\\pmb{q}对应的向量旋转m\\theta角度： \\begin{aligned} \\pmb{f}(\\pmb{q}, m) &= \\pmb{q}\\cdot e^{im\\theta} \\\\ &= \\left[ \\begin{matrix} cosm\\theta & -sinm\\theta \\\\ sinm\\theta & cosm\\theta \\end{matrix} \\right] \\left[ \\begin{matrix} q_0 \\\\ q_1 \\end{matrix} \\right] \\end{aligned} \\tag{10} 由于内积满足线性叠加性，因此任意偶数维的RoPE可以表示为二维情形的拼接： \\left[ \\begin{matrix} cosm\\theta_0 & -sinm\\theta_0 & 0 & 0 & ... & 0 & 0\\\\ sinm\\theta_0 & cosm\\theta_0 & 0 & 0 & ... & 0 & 0\\\\ 0 & 0 & cosm\\theta_1 & -sinm\\theta_1 & ... & 0 & 0\\\\ 0 & 0 & sinm\\theta_1 & cosm\\theta_1 & ... & 0 & 0\\\\ ... & ... & ... & ... & ... & ... & ...\\\\ 0 & 0 & ... & 0 & 0 & cosm\\theta_{d/2-1} & -sinm\\theta_{d/2-1}\\\\ 0 & 0 & ... & 0 & 0 & sinm\\theta_{d/2-1} & cosm\\theta_{d/2-1}\\\\ \\end{matrix} \\right] \\left[ \\begin{matrix} q_0 \\\\ q_1 \\\\ q_2 \\\\ q_3 \\\\ ... \\\\ q_{d-2} \\\\ q_{d-1} \\end{matrix} \\right] \\tag{11} (11)式的左边就是R_m。 给位置为m的向量\\pmb{q}乘上矩阵R_m，位置为n的向量\\pmb{k}乘上矩阵R_n，用变换后的\\pmb{Q}，\\pmb{K}序列作Attention，那么Attention自动包含相对位置信息，因为成立恒等式： \\begin{aligned} (R_m\\pmb{q})^T(R_n\\pmb{k}) &= \\pmb{q}^TR_m^TR_n\\pmb{k} \\\\ &= \\pmb{q}^TR_{n-m}\\pmb{k} \\end{aligned} \\tag{12} 其中(12)式从第一行到第二行，利用三角恒等式即可推导出来，很简单。 其实，(11)式的矩阵相乘已经可以用代码实现了，但是R_m是个稀疏矩阵，所以直接用矩阵乘法来实现会很浪费算力，因此将其改为如下形式： \\begin{aligned} \\left[ \\begin{matrix} q_0 \\\\ q_1 \\\\ q_2 \\\\ q_3 \\\\ ... \\\\ q_{d-2} \\\\ q_{d-1} \\end{matrix} \\right] \\bigotimes \\left[ \\begin{matrix} cosm\\theta_{0} \\\\ cosm\\theta_{0} \\\\ cosm\\theta_{1} \\\\ cosm\\theta_{1} \\\\ ... \\\\ cosm\\theta_{d/2-1} \\\\ cosm\\theta_{d/2-1} \\end{matrix} \\right] + \\left[ \\begin{matrix} -q_1 \\\\ q_0 \\\\ -q_3 \\\\ q_2 \\\\ ... \\\\ -q_{d-1} \\\\ q_{d-2} \\end{matrix} \\right] \\bigotimes \\left[ \\begin{matrix} sinm\\theta_{0} \\\\ sinm\\theta_{0} \\\\ sinm\\theta_{1} \\\\ sinm\\theta_{1} \\\\ ... \\\\ sinm\\theta_{d/2-1} \\\\ sinm\\theta_{d/2-1} \\end{matrix} \\right] \\end{aligned} \\tag{13} 因此RoPE的核心代码就是(13)式： class GlobalPointer(layer): def call(self, inputs, mask=None): # 输入变换 inputs = self.dense(inputs) # 全链接：q = Wh + b # 针对每一个实体类别进行拆分，拆分完的结果是q_alpha, k_alpha序列 inputs = tf.split(inputs, self.heads, axis=-1) inputs = K.stack(inputs, axis=-2) qw, kw = inputs[..., :self.head_size], inputs[..., self.head_size:] if self.RoPE: pos = SinusoidalPositionEmbedding(self.head_size, 'zero')(inputs) # 对应公式(13)中的cos部分 cos_pos = K.repeat_elements(pos[..., None, 1::2], 2, -1) # 对应公式(13)中的sin部分 sin_pos = K.repeat_elements(pos[..., None, ::2], 2, -1) # qw2对应公式(13)中的(-q1, q0, -q3, q2, ...) qw2 = K.stack([-qw[..., 1::2], qw[..., ::2]], 4) qw2 = K.reshape(qw2, K.shape(qw)) qw = qw * cos_pos + qw2 * sin_pos kw2 = K.stack([-kw[..., 1::2], kw[..., ::2]], 4) kw2 = K.reshape(kw2, K.shape(kw)) kw = kw * cos_pos + kw2 * sin_pos # 计算内积 # qw和kw是包含了绝对位置信息的张量，点积运算使其获得相对位置信息 logits = tf.einsum('bmhd,bnhd->bhmn', qw, kw) # 排除padding logits = sequence_masking(logits, mask, '-inf', 2) logits = sequence_masking(logits, mask, '-inf', 3) # 排除下三角 if self.tril_mask: mask = tf.linalg.band_part(K.ones_like(logits), 0, -1) logits = logits - (1 - mask) * K.infinity() # scale返回 # 注意博客中写globalpointer原理的时候并没有scale操作，实现时是有的 return logits / self.head_size**0.5 基础知识复习 1. 复数的各种形式 复平面 复平面中的点(a, b)对应复数a+bi，其共轭为a-bi 三角形式： \\begin{aligned} a &= rcos\\theta \\\\ b &= rsin\\theta \\\\ a + bi &= rcos\\theta + risin\\theta \\\\ &= r(cos\\theta + isin\\theta) \\end{aligned} 指数形式（使用三角形式的乘法进行推导）：a+bi=re^{i\\theta} 其共轭为re^{-i\\theta} 2. 复数相乘的几何意义 以下证明复数相乘的几何意义为旋转。 我们已知复数的三角形式，假设有复数z_1和z_2，它们的三角形式为： \\begin{aligned} z_1 &= r_1(cos\\theta_1 + isin\\theta_1) \\\\ z_2 &= r_2(cos\\theta_2 + isin\\theta_2) \\\\ 对应复平面\\Rightarrow \\pmb{OZ}_1 &= (r_1cos\\theta_1, r_1sin\\theta_1) \\\\ \\pmb{OZ}_2&= (r_2cos\\theta_2, r_2sin\\theta_2) \\end{aligned} 从两边向中间证， 左边： \\begin{aligned} z_1z_2 &= r_1r_2(cos\\theta_1 + isin\\theta_1)(cos\\theta_2 + isin\\theta_2) \\\\ &= r_1r_2(cos\\theta_1cos\\theta_2+icos\\theta_1sin\\theta_2+isin\\theta_1cos\\theta_2-sin\\theta_1sin\\theta_2) \\\\ &= r_1r_2[cos(\\theta_1+\\theta_2)+isin(\\theta_1+\\theta_2)] \\end{aligned} 右边： \\begin{aligned} \\pmb{OZ}_1 &= (r_1cos\\theta_1, r_1sin\\theta_1) \\end{aligned} 对\\pmb{OZ}_1旋转\\theta_2并缩放r_2： \\begin{aligned} r_2\\cdot \\left[ \\begin{matrix} cos\\theta_2 & -sin\\theta_2 \\\\ sin\\theta_2 & cos\\theta_2 \\end{matrix} \\right] \\left[ \\begin{matrix} r_1cos\\theta_1 \\\\ r_1sin\\theta_1 \\end{matrix} \\right] &= r_2\\cdot \\left[ \\begin{matrix} r_1cos\\theta_1cos\\theta_2 - r_1sin\\theta_1sin\\theta_2 \\\\ r_1cos\\theta_1sin\\theta_2 + r_1cos\\theta_2sin\\theta_1 \\end{matrix} \\right] \\\\ &= r_1r_2\\left[ \\begin{matrix} cos(\\theta_1+\\theta_2) \\\\ sin(\\theta_1+\\theta_2) \\end{matrix} \\right] \\end{aligned} 由此证明出，z_1z_2的几何意义是：把复数z_1对应的向量\\pmb{OZ}_1，绕O旋转\\theta_2，然后再缩放r_2对应的复数。 3. 等差数列通项 {a_n}为等差数列，那么有等差数列通项： a_n = a_1 + (n-1) \\times d "},"book/papers/多标签CE.html":{"url":"book/papers/多标签CE.html","title":"多标签CE","keywords":"","body":"多标签CrossEntropy 对于多标签分类问题，一个简单且直观的想法是，将其转换成多个二分类问题。具体来说，就是把“n个类别选k个目标类别”的问题转化为“n个二分类”问题，然后将n个二分类交叉熵之和作为损失。这种做法会遇到的问题是，一般尤其在序列标注任务中n \\gg k，会面临严重的类别不均衡问题，那么我们就需要一些后续的平衡策略，比如，手动调整正负样本的权重；使用focal loss等等。 其实，n选k应该是n选1的自然延伸，却需要多做那么多工作，是不科学的事情。 首先考虑多分类单标签问题，即从n个候选类别中选1个目标类别。假设各个类的得分为s_1, s_2, ... s_n，目标类为t \\in {1, 2, ..., n}，那么损失为： -log\\frac {e^{s_t}}{\\sum _{i=1}^ne^{s_i}} = -s_t+log\\sum _{i=1}^ne^{s_i} 换一种方式看单标签分类交叉熵： \\begin{aligned} -log\\frac {e^{s_t}}{\\sum _{i=1}^ne^{s_i}} &= -log\\frac {1}{\\sum _{i=1}^n e^{s_i-s_t}} \\\\ &= log \\sum _{i=1}^n e^{s_i-s_t} \\\\ &= log(1+\\sum _{i=1,i\\not=t}^ne^{s_i-s_t}) \\end{aligned} 因为logsumexp是max的光滑近似，因此： log(1+\\sum _{i=1,i\\not=t}^ne^{s_i-s_t}) \\approx max \\left[ \\begin{matrix} 0 \\\\ s_1-s_t \\\\ s_2-s_t \\\\ ... \\\\ s_{t-1}-s_t \\\\ s_{t+1}-s_t \\\\ ... \\\\ s_n-s_t \\end{matrix} \\right] 该损失的特点为，所有的非目标类得分\\{s_1, s_2, ..., s_{t-1}, s_{t+1},...,s_n\\}和目标类得分\\{s_t\\}两两做差比较，它们差的最大值都要尽可能小于零，因此实现了“目标类得分都大于每个非目标类的得分”的效果。 如果有多个目标类的多标签分类场景，我们也希望“每个目标类得分都不小于每个非目标类的得分”，因此loss有如下形式： log(1 + \\sum _{i\\in \\Omega_{neg},j\\in \\Omega_{pos}}e^{s_i-s_j}) = log(1 + \\sum _{i\\in \\Omega_{neg}}e^{s_i}\\cdot \\sum _{j\\in \\Omega_{pos}}e^{-s_j}) \\tag{1} 即我们希望s_i。 n类选k类，如果k是固定的，那么直接用式(1)即可，预测时直接输出得分最大的k个类别。 对于k 不固定的多标签分类，需要一个阈值来确定输出哪些类，因此引入一个0类，希望目标类的分数都大于s_0，非目标类分数都小于s_0。从(1)式我们知道，“希望s_i就在log里边加入e^{s_i-s_j}”，因此(1)式变为： log(1 + \\sum _{i\\in \\Omega_{neg},j\\in \\Omega_{pos}}e^{s_i-s_j} + \\sum _{i\\in \\Omega_{neg}}e^{s_i-s_0} + \\sum _{j\\in \\Omega_{pos}}e^{s_0-s_j}) \\\\ \\\\ \\begin{aligned} &= log(e^{s_0-s_0} + \\sum _{i\\in \\Omega_{neg},j\\in \\Omega_{pos}}e^{s_i-s_j} + \\sum _{i\\in \\Omega_{neg}}e^{s_i-s_0} + \\sum _{j\\in \\Omega_{pos}}e^{s_0-s_j}) \\\\ \\\\ &= log[e^{s_0}(e^{-s_0} + \\sum _{j\\in \\Omega_{pos}}e^{-s_j}) + \\sum _{i\\in \\Omega_{neg}}e^{s_i}(\\sum _{j\\in \\Omega_{pos}}e^{-s_j}+e^{-s_0})] \\\\ \\\\ &= log[(\\sum _{j\\in \\Omega_{pos}}e^{-s_j}+e^{-s_0})(\\sum _{i\\in \\Omega_{neg}}e^{s_i}+e^{s_0})] \\\\ \\\\ &= log(\\sum _{j\\in \\Omega_{pos}}e^{-s_j}+e^{-s_0}) + log(\\sum _{i\\in \\Omega_{neg}}e^{s_i}+e^{s_0}) \\end{aligned} \\tag{2} 指定s_0=0，那么(2)式为： log(1 + \\sum _{j\\in \\Omega_{pos}}e^{-s_j}) + log(1 + \\sum _{i\\in \\Omega_{neg}}e^{s_i}) \\tag{3} 预测时取s_k>0为正例。 代码实现完全就是公式(3): def multilabel_categorical_crossentropy(y_true, y_pred): \"\"\"多标签分类的交叉熵 说明： 1. y_true和y_pred的shape一致，y_true的元素非0即1， 1表示对应的类为目标类，0表示对应的类为非目标类； 2. 请保证y_pred的值域是全体实数，换言之一般情况下 y_pred不用加激活函数，尤其是不能加sigmoid或者 softmax； 3. 预测阶段则输出y_pred大于0的类； \"\"\" y_pred = (1 - 2 * y_true) * y_pred # 把正标签的分数变负号 y_neg = y_pred - y_true * K.infinity() # 把负标签取出来，正标签位置对应负无穷，在logsumexp计算时相当于没有，即logsumexp([a, b, c]) == logsumexp([a, b, c, -无穷]) y_pos = y_pred - (1 - y_true) * K.infinity() # 与上一行同理 zeros = K.zeros_like(y_pred[..., :1]) y_neg = K.concatenate([y_neg, zeros], axis=-1) # e的0次方等于1 y_pos = K.concatenate([y_pos, zeros], axis=-1) neg_loss = K.logsumexp(y_neg, axis=-1) # axis == -1 表示在最后一个维度上进行计算 pos_loss = K.logsumexp(y_pos, axis=-1) return neg_loss + pos_loss s_\\alpha(i, j)=(\\pmb{W}_q\\pmb{h}_i)^T(\\pmb{W}_k\\pmb{h}_j)+\\pmb{w}_\\alpha^T[\\pmb{h}_i;\\pmb{h}_j] "}}